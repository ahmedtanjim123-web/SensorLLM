{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import torch"
      ],
      "metadata": {
        "id": "LDvU-nMW3p0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "source": [
        "def check_label_continuity(df):\n",
        "    continuity_segments = {}\n",
        "\n",
        "    for subject in df['subject'].unique():\n",
        "        subject_data = df[df['subject'] == subject]\n",
        "        assert subject_data.index[0]==0\n",
        "\n",
        "        for label in subject_data['activity'].unique():\n",
        "            label_data = subject_data[subject_data['activity'] == label]\n",
        "\n",
        "            indices = label_data.index\n",
        "            segments = []\n",
        "            start_idx = indices[0]\n",
        "\n",
        "            for i in range(len(indices) - 1):\n",
        "                if indices[i] + 1 != indices[i + 1]:\n",
        "                    end_idx = indices[i]\n",
        "                    segments.append((start_idx, end_idx))\n",
        "                    start_idx = indices[i + 1]\n",
        "\n",
        "            segments.append((start_idx, indices[-1]))\n",
        "\n",
        "            if segments:\n",
        "                continuity_segments[(subject, label)] = segments\n",
        "\n",
        "    return continuity_segments"
      ],
      "metadata": {
        "id": "8xBfEy4m3p0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "def generate_window_sizes(sequence_length, window_size_list):\n",
        "    min_window_size, max_window_size = window_size_list[0], window_size_list[1]\n",
        "    window_sizes = []\n",
        "    remaining_length = sequence_length\n",
        "    while remaining_length > 0:\n",
        "        if remaining_length < min_window_size:\n",
        "            if window_sizes:\n",
        "                last_window_size = window_sizes.pop()\n",
        "                remaining_length += last_window_size\n",
        "                continue\n",
        "            else:\n",
        "                raise ValueError(\"没有之前生成的窗口大小\")\n",
        "        window_size = random.randint(min_window_size, min(max_window_size, remaining_length))\n",
        "        window_sizes.append(window_size)\n",
        "        remaining_length -= window_size\n",
        "    return window_sizes\n",
        "\n",
        "\n",
        "def split_sequences(sequences, window_size_list, n=1):\n",
        "    assert len(sequences[0]) == 15\n",
        "    # 检查是否存在空值\n",
        "    has_null = any(any(pd.isnull(item) or item == '' for item in sublist) for sublist in sequences)\n",
        "\n",
        "    # 输出结果\n",
        "    if has_null:\n",
        "        raise ValueError(\"Has null values。\")\n",
        "\n",
        "    sequence_length = len(sequences)\n",
        "    segments = []\n",
        "    labels_set = set()\n",
        "    labels = []\n",
        "    for _ in range(n):\n",
        "        window_sizes = generate_window_sizes(sequence_length, window_size_list)\n",
        "\n",
        "        start = 0\n",
        "        for window_size in window_sizes:\n",
        "            end = start + window_size\n",
        "            if (start, end) not in labels_set:\n",
        "                labels_set.add((start, end))\n",
        "                segment = sequences[start:end]\n",
        "                segments.append(np.array(segment))\n",
        "                labels.append([start, end])\n",
        "            start = end\n",
        "    return segments, labels"
      ],
      "metadata": {
        "id": "0qwIEbBm3p0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-19 16:38:51--  https://archive.ics.uci.edu/ml/machine-learning-databases/00319/MHEALTHDATASET.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘MHEALTHDATASET.zip’\n",
            "\n",
            "MHEALTHDATASET.zip      [     <=>            ]  72.07M  61.2MB/s    in 1.2s    \n",
            "\n",
            "2025-11-19 16:38:53 (61.2 MB/s) - ‘MHEALTHDATASET.zip’ saved [75567983]\n",
            "\n",
            "Archive:  MHEALTHDATASET.zip\n",
            "   creating: mhealth/MHEALTHDATASET/\n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject1.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject10.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject2.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject3.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject4.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject5.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject6.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject7.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject8.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/mHealth_subject9.log  \n",
            "  inflating: mhealth/MHEALTHDATASET/README.txt  \n",
            "all_train_segments: 9571\n",
            "all_train_labels: 9571\n",
            "all_test_segments: 2036\n",
            "all_test_labels: 2036\n"
          ]
        }
      ],
      "source": [
        "activity_map = {\n",
        "    1: 'Standing still (1 min)',\n",
        "    2: 'Sitting and relaxing (1 min)',\n",
        "    3: 'Lying down (1 min)',\n",
        "    4: 'Walking (1 min)',\n",
        "    5: 'Climbing stairs (1 min)',\n",
        "    6: 'Waist bends forward (20x)',\n",
        "    7: 'Frontal elevation of arms (20x)',\n",
        "    8: 'Knees bending (crouching) (20x)',\n",
        "    9: 'Cycling (1 min)',\n",
        "    10: 'Jogging (1 min)',\n",
        "    11: 'Running (1 min)',\n",
        "    12: 'Jump front & back (20x)'\n",
        "}\n",
        "test_id = ['subject1', 'subject3', 'subject6']\n",
        "window_size=[5, 100]\n",
        "\n",
        "all_train_segments = []\n",
        "all_test_segments = []\n",
        "all_train_labels = []\n",
        "all_test_labels = []\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00319/MHEALTHDATASET.zip\n",
        "!unzip MHEALTHDATASET.zip -d mhealth\n",
        "\n",
        "import pandas as pd\n",
        "for i in range(1, 11):\n",
        "    df = pd.read_csv(f'mhealth/MHEALTHDATASET/mHealth_subject{i}.log', header=None, sep='\\t')\n",
        "    # Note: Excluding the ECG data collected with the chest sensor\n",
        "    df = df.loc[:, [0, 1, 2, 5, 6, 7, 8, 9, 10, 14, 15, 16, 17, 18, 19, 23]].rename(columns= {\n",
        "        0: 'acc_ch_x',\n",
        "        1: 'acc_ch_y',\n",
        "        2: 'acc_ch_z',\n",
        "        5: 'acc_la_x',\n",
        "        6: 'acc_la_y',\n",
        "        7: 'acc_la_z',\n",
        "        8: 'gyr_la_x',\n",
        "        9: 'gyr_la_y',\n",
        "        10: 'gyr_la_z',\n",
        "        14: 'acc_rw_x',\n",
        "        15: 'acc_rw_y',\n",
        "        16: 'acc_rw_z',\n",
        "        17: 'gyr_rw_x',\n",
        "        18: 'gyr_rw_y',\n",
        "        19: 'gyr_rw_z',\n",
        "        23: 'activity'\n",
        "    })\n",
        "    df['subject'] = f'subject{i}'\n",
        "    continuity_segments = check_label_continuity(df)\n",
        "    for key, value in continuity_segments.items():\n",
        "        if key[1] == 0: # class != 1\n",
        "            continue\n",
        "        for segment in value:\n",
        "            # 划分时间序列数据为片段\n",
        "            rows = df.loc[segment[0]:segment[1]]\n",
        "\n",
        "            assert len(rows['subject'].unique()) == 1\n",
        "            assert rows['subject'].unique()[0] == key[0]\n",
        "            assert len(rows['activity'].unique()) == 1, f\"Subject {key[0]}, activity {key[1]} but has {rows['activity'].unique()},  {segment[0]} 到 {segment[1]}\"\n",
        "            assert rows['activity'].unique()[0] == key[1]\n",
        "\n",
        "            subject_activity_df = rows.iloc[:, ~rows.columns.isin(['subject', 'activity'])]\n",
        "            subject_activity_series = subject_activity_df.values.tolist()\n",
        "\n",
        "            if key[0] not in test_id:\n",
        "                segments, labels = split_sequences(subject_activity_series, window_size, 2)\n",
        "                all_train_segments.extend(segments)\n",
        "            else:\n",
        "                segments, labels = split_sequences(subject_activity_series, window_size, 1)\n",
        "                all_test_segments.extend(segments)\n",
        "\n",
        "            for label in labels:\n",
        "                label_dict = {\n",
        "                    \"subject\": key[0],\n",
        "                    \"activity_name\": activity_map[key[1]],\n",
        "                    \"activity\": key[1]-1,\n",
        "                    \"segments\": label\n",
        "                }\n",
        "                if key[0] not in test_id:\n",
        "                    all_train_labels.append(label_dict)\n",
        "                else:\n",
        "                    all_test_labels.append(label_dict)\n",
        "\n",
        "print(f\"all_train_segments: {len(all_train_segments)}\")\n",
        "print(f\"all_train_labels: {len(all_train_labels)}\")\n",
        "print(f\"all_test_segments: {len(all_test_segments)}\")\n",
        "print(f\"all_test_labels: {len(all_test_labels)}\")"
      ],
      "metadata": {
        "id": "75RW4-wj3p0F",
        "outputId": "ca6a5359-b2f7-4a7a-8c87-919079056c4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Skipping (already exists): /content/drive/MyDrive/sensorllm/whole_data/train/mhealth_train_data_stage1.pkl\n",
            "Skipping (already exists): /content/drive/MyDrive/sensorllm/whole_data/test/mhealth_test_data_stage1.pkl\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/sensorllm/whole_data\"\n",
        "os.makedirs(os.path.join(output_path, \"train\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(output_path, \"test\"), exist_ok=True)\n",
        "\n",
        "def save_if_not_exists(path, data):\n",
        "    if os.path.exists(path):\n",
        "        print(f\"Skipping (already exists): {path}\")\n",
        "        return\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "        print(f\"Saved: {path}\")\n",
        "\n",
        "save_if_not_exists(os.path.join(output_path, 'train', 'mhealth_train_data_stage1.pkl'), all_train_segments)\n",
        "save_if_not_exists(os.path.join(output_path, 'test', 'mhealth_test_data_stage1.pkl'), all_test_segments)"
      ],
      "metadata": {
        "id": "6lI0kp0E3p0G",
        "outputId": "48cb3233-1ade-4a93-aa26-1e8dea9afda6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "source": [
        "PROMPT_DICT = {\n",
        "    \"trend_synonyms\": {\n",
        "        \"upward\": \"downward\",\n",
        "        \"ascending\": \"descending\",\n",
        "        \"rising\": \"falling\",\n",
        "        \"increasing\": \"decreasing\",\n",
        "        \"growing\": \"declining\"\n",
        "    },\n",
        "    \"steady_synonyms\": [\n",
        "        \"steady\",\n",
        "        \"constant\",\n",
        "        \"stable\",\n",
        "        \"consistent\"\n",
        "    ],\n",
        "    \"gen_smry_q\": [\n",
        "        \"Could you provide a summary of the main features of the input {data} and the distribution of the trends?\",\n",
        "        \"Please give an overview of the essential attributes of the input {data} and the spread of the trends.\",\n",
        "        \"I would appreciate if you could outline the primary characteristics of the input {data} and the distribution of the trends.\",\n",
        "        \"Can you present a brief description of the fundamental properties of the input {data} and the allocation of the trends?\",\n",
        "        \"Would you be able to summarize the significant aspects of the input {data} and the dispersion of the trends?\",\n",
        "        \"I kindly request a concise report on the central qualities of the input {data} and the distribution of the trends.\",\n",
        "        \"Please provide a succinct account of the crucial elements of the input {data} and the distribution of the trends.\",\n",
        "        \"Please provide a summary of the main features of the input {data} and the trends observed in its distribution.\",\n",
        "        \"Could you analyze the key aspects of the {data} input and outline the distribution trends?\",\n",
        "        \"I need an overview of the primary characteristics of the input {data} and a description of the trend distribution.\",\n",
        "        \"Summarize the essential elements of the input {data} and the patterns in its distribution.\",\n",
        "        \"Explain the fundamental attributes of the {data} input and the distribution trends it exhibits.\",\n",
        "        \"Can you break down the main features and distribution trends of the input {data}?\",\n",
        "        \"Offer a concise summary of the input {data}'s key characteristics and how its trends distribute.\",\n",
        "        \"Detail the core aspects and distribution patterns observed in the {data} input.\",\n",
        "        \"Identify and describe the key features and trend distribution within the input {data}.\",\n",
        "        \"Provide insights into the primary elements and distribution trends of the {data} input.\",\n",
        "        \"Examine the principal attributes of the {data} input and report on the observed distribution trends.\",\n",
        "        \"Highlight the significant characteristics of the input {data} and the nature of its trend distribution.\",\n",
        "        \"Can you summarize the key aspects of {data} and the trend distribution?\",\n",
        "        \"Please outline the primary characteristics of {data} and the trend patterns.\",\n",
        "        \"Could you detail the main features of {data} and outline the trend distribution?\",\n",
        "        \"I need a summary of {data}'s main elements and their trend distributions.\",\n",
        "        \"Please provide insights into the core features of {data} and the distribution of trends.\",\n",
        "        \"Can you highlight the principal components of {data} and their trend distribution?\",\n",
        "        \"Summarize the essential aspects of {data} and the trends' distribution, please.\",\n",
        "        \"Summarize the key features and trend distribution of the {data}.\",\n",
        "        \"What are the main characteristics and trend patterns in the {data}?\",\n",
        "        \"Describe the primary attributes and trend dispersion of the {data}.\",\n",
        "        \"Provide an overview of the {data}'s main features and trend distribution.\",\n",
        "        \"Explain the essential properties and trend spread of the {data}.\",\n",
        "        \"Outline the principal aspects and trend allocation of the {data}.\",\n",
        "        \"Summarize the {data}'s core features and trend dissemination.\",\n",
        "        \"What are the fundamental traits and trend arrangement in the {data}?\",\n",
        "        \"Give a summary of the {data}'s main elements and trend apportionment.\",\n",
        "        \"Describe the salient features and trend distribution within the {data}.\"\n",
        "    ],\n",
        "    \"gen_summary_1\": [\n",
        "        \"The given {data_name} representing the {sensor_name} sensor readings from {start_time}s to {end_time}s.\",\n",
        "        \"The {data_name} represents readings taken from an {sensor_name} sensor between {start_time} and {end_time} seconds.\",\n",
        "        \"This {data_name} comprises {sensor_name} sensor readings collected from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"The {sensor_name} sensor readings recorded within the {start_time} to {end_time} second timeframe are presented in this {data_name}.\",\n",
        "        \"The {data_name} encapsulates {sensor_name} sensor readings from {start_time}s to {end_time}s.\",\n",
        "        \"Readings from an {sensor_name} sensor, captured from {start_time} seconds to {end_time} seconds, are depicted in the given {data_name}.\",\n",
        "        \"The {data_name} illustrates measurements from an {sensor_name} sensor between {start_time} and {end_time} seconds.\",\n",
        "        \"Presented is a span of {data_name}, indicating readings from an {sensor_name} sensor taken within the {start_time} to {end_time} second timeframe.\",\n",
        "        \"This {data_name} reflects the output from an {sensor_name} sensor, measured from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"The presented {data_name} depicts the measurements obtained from an {sensor_name} sensor between {start_time} and {end_time} seconds.\",\n",
        "        \"The {data_name} provided represents the output of an {sensor_name} sensor recorded between {start_time}s and {end_time}s.\",\n",
        "        \"The {data_name} corresponds to the readings collected from an {sensor_name} sensor between {start_time}s and {end_time}s.\",\n",
        "        \"The {data_name} illustrates the {sensor_name} sensor's measurements captured from {start_time}s to {end_time}s.\",\n",
        "        \"The given {data_name} represents the {sensor_name} sensor's output recorded between {start_time} and {end_time} seconds.\",\n",
        "        \"The {data_name} showcases the {sensor_name} sensor's readings acquired between {start_time}s and {end_time}s.\",\n",
        "        \"The {data_name} represent the {sensor_name} sensor's measurements taken from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"The {data_name} encapsulates the {sensor_name} sensor's output collected between {start_time} and {end_time} seconds.\",\n",
        "        \"The {data_name} comprises the {sensor_name} sensor's readings gathered from {start_time}s to {end_time}s.\",\n",
        "        \"The {data_name} exhibits the {sensor_name} sensor's measurements registered within the {start_time} to {end_time} second timeframe.\",\n",
        "        \"The {data_name} displays readings obtained from an {sensor_name} sensor from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"Readings collected from an {sensor_name} sensor from {start_time}s to {end_time}s are documented in this {data_name}.\",\n",
        "        \"This {data_name} encapsulates the readings from an {sensor_name} sensor between {start_time} and {end_time} seconds.\",\n",
        "        \"The provided {data_name} captures the readings from an {sensor_name} sensor, recorded between {start_time} and {end_time} seconds.\",\n",
        "        \"This {data_name} represents the readings from an {sensor_name} sensor between {start_time}s and {end_time}s.\",\n",
        "        \"Readings from an {sensor_name} sensor between {start_time}s and {end_time}s are chronicled in the given {data_name}.\",\n",
        "        \"The {data_name} illustrates {sensor_name} sensor readings between {start_time} and {end_time} seconds.\",\n",
        "        \"Recordings from an {sensor_name} sensor, between {start_time} and {end_time} seconds, are conveyed in this {data_name}.\",\n",
        "        \"The {data_name} provided is a representation of the {sensor_name} sensor's output recorded continuously within the {start_time} to {end_time} second timeframe.\",\n",
        "        \"The presented {data_name} encapsulates the {sensor_name} sensor's readings collected sequentially between {start_time}s and {end_time}s.\",\n",
        "        \"The {data_name} under consideration contains the {sensor_name} sensor's output captured from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"The provided {data_name} shows readings from the {sensor_name} sensor from {start_time}s to {end_time}s.\",\n",
        "        \"The {data_name} contains {sensor_name} sensor data between {start_time}s and {end_time}s.\",\n",
        "        \"Readings from the {sensor_name} sensor between {start_time}s and {end_time}s are in the {data_name}.\",\n",
        "        \"{data_name} includes {sensor_name} sensor observations taken from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"The {data_name} shows {sensor_name} readings from {start_time}s to {end_time}s.\",\n",
        "        \"{sensor_name} sensor data between {start_time}s and {end_time}s is represented in {data_name}.\",\n",
        "        \"{data_name} presents {sensor_name} data collected between {start_time} and {end_time} seconds.\",\n",
        "        \"{sensor_name} readings between {start_time}s and {end_time}s are displayed in {data_name}.\",\n",
        "        \"{sensor_name} sensor readings are captured in {data_name} within the {start_time} to {end_time} second timeframe.\",\n",
        "        \"From {start_time}s to {end_time}s, {sensor_name} data is showcased in the {data_name}.\",\n",
        "        \"The {data_name} exhibits {sensor_name} readings from {start_time} seconds to {end_time} seconds.\"\n",
        "    ],\n",
        "    \"gen_summary_2\": [\n",
        "        \"The data exhibits {trend_num} distinct trends, with a total of {change_num} changes in trend observed.\",\n",
        "        \"Analysis reveals {trend_num} separate trends within the data, undergoing a cumulative total of {change_num} shifts in direction.\",\n",
        "        \"There are {trend_num} unique trends identified in the data, which altogether have shifted direction {change_num} times.\",\n",
        "        \"The data outlines {trend_num} different patterns, with these patterns changing direction a total of {change_num} times.\",\n",
        "        \"{trend_num} varied trends have been observed in the data, which altogether experienced {change_num} transitions.\",\n",
        "        \"The input data displays {trend_num} individual trends, with a comprehensive change count reaching {change_num}.\",\n",
        "        \"In the data, {trend_num} distinct movement trends are evident, and there have been {change_num} total trend alterations.\",\n",
        "        \"The data delineates {trend_num} unique trends, undergoing {change_num} total changes in these trends.\",\n",
        "        \"{trend_num} separate trends can be discerned within the data, with a total of {change_num} instances of trend modification.\",\n",
        "        \"The data shows {trend_num} different trajectories, with these trajectories having changed a total of {change_num} times.\",\n",
        "        \"Analysis of the data shows {trend_num} main trend patterns, and the trend has undergone {change_num} shifts in total.\",\n",
        "        \"The data highlights {trend_num} significant trends, while also indicating that the trend has changed {change_num} times overall.\",\n",
        "        \"Overall, the data reflects {trend_num} different development trends, which have experienced {change_num} changes in total.\",\n",
        "        \"The data demonstrates {trend_num} major trend types, with the trend undergoing {change_num} turning points during the entire period.\",\n",
        "        \"Examining the data, we notice {trend_num} clear trend characteristics, with the trend fluctuating a total of {change_num} times.\",\n",
        "        \"The data mirrors {trend_num} different development tendencies, while also illustrating that the trend has changed {change_num} times in total.\",\n",
        "        \"From a holistic perspective, the data presents {trend_num} unique trend forms, which have undergone {change_num} changes throughout the process.\",\n",
        "        \"The data indicates {trend_num} primary shifting trends, with these trends transforming a total of {change_num} times.\",\n",
        "        \"Parsing through the data, we discover {trend_num} distinct trend features, with the trend varying {change_num} times over the entire period.\",\n",
        "        \"There are {trend_num} unique trends and {change_num} total trend changes observed in the data.\",\n",
        "        \"The data shows {trend_num} different trends, with {change_num} changes in these trends.\",\n",
        "        \"Analysis reveals {trend_num} separate trends and a total of {change_num} shifts in trend direction.\",\n",
        "        \"We identified {trend_num} distinct patterns, along with {change_num} overall changes in trends in the given data.\",\n",
        "        \"The input data demonstrates {trend_num} unique trends, experiencing {change_num} trend alterations in total.\",\n",
        "        \"Observation indicates {trend_num} different trends with {change_num} instances of trend changes.\",\n",
        "        \"The analysis points to {trend_num} distinct trends and {change_num} changes in the trends.\",\n",
        "        \"{trend_num} separate trends and {change_num} trend shifts are seen in the data.\",\n",
        "        \"The data reveals {trend_num} distinct trends with {change_num} trend variations.\",\n",
        "        \"The data displays {trend_num} individual trends and {change_num} trend fluctuations.\",\n",
        "        \"{change_num} trend changes are observed across {trend_num} trends in the input data.\",\n",
        "        \"The data contains {trend_num} trends, exhibiting {change_num} trend modifications.\",\n",
        "        \"{trend_num} trends are present in the data, with {change_num} instances of trend changes.\",\n",
        "        \"Across {trend_num} trends, the data shows {change_num} occurrences of trend shifts.\"\n",
        "    ],\n",
        "    \"gen_summary_2_2\": [\n",
        "        \"The data exhibits {trend_num} distinct trend.\",\n",
        "        \"Analysis reveals {trend_num} separate trend within the data.\",\n",
        "        \"There is {trend_num} unique trend identified in the data.\",\n",
        "        \"The data outlines {trend_num} pattern.\",\n",
        "        \"{trend_num} varied trend has been observed in the data.\",\n",
        "        \"The input data displays {trend_num} individual trend.\",\n",
        "        \"In the data, {trend_num} distinct movement trend is evident.\",\n",
        "        \"The data delineates {trend_num} unique trend.\",\n",
        "        \"{trend_num} separate trend can be discerned within the data.\",\n",
        "        \"The data shows {trend_num} different trajectory.\",\n",
        "        \"Analysis of the data shows {trend_num} main trend pattern.\",\n",
        "        \"The data highlights {trend_num} significant trend.\",\n",
        "        \"Overall, the data reflects {trend_num} different development trend.\",\n",
        "        \"The data demonstrates {trend_num} trend type.\",\n",
        "        \"Examining the data, we notice {trend_num} clear trend characteristic.\",\n",
        "        \"The data mirrors {trend_num} different development tendency.\",\n",
        "        \"From a holistic perspective, the data presents {trend_num} unique trend form.\",\n",
        "        \"The data indicates {trend_num} primary shifting trend.\",\n",
        "        \"Parsing through the data, we discover {trend_num} distinct trend feature.\",\n",
        "        \"There is {trend_num} unique trend observed in the data.\",\n",
        "        \"The data shows {trend_num} different trend.\",\n",
        "        \"Analysis reveals {trend_num} separate trend.\",\n",
        "        \"We identified {trend_num} distinct pattern.\",\n",
        "        \"The input data demonstrates {trend_num} unique trend.\",\n",
        "        \"Observation indicates {trend_num} different trend.\",\n",
        "        \"The analysis points to {trend_num} distinct trend.\",\n",
        "        \"{trend_num} separate trend is seen in the data.\",\n",
        "        \"The data reveals {trend_num} distinct trend.\",\n",
        "        \"The data displays {trend_num} individual trend.\",\n",
        "        \"{trend_num} trend is observed in the input data.\",\n",
        "        \"The data contains {trend_num} trend.\",\n",
        "        \"{trend_num} trend is present in the data.\"\n",
        "    ],\n",
        "    \"gen_summary_3\": [\n",
        "        \"To sum up, the data exhibited a {trend_type} trend for a cumulative period of {total_time} seconds\",\n",
        "        \"In conclusion, the overall timespan of the data's {trend_type} tendency amounted to {total_time} seconds\",\n",
        "        \"Summarizing the findings, the aggregate time during which the data displayed a {trend_type} pattern was {total_time} seconds\",\n",
        "        \"The analysis reveals that the data's {trend_type} inclination persisted for a total of {total_time} seconds\",\n",
        "        \"To encapsulate, the data's {trend_type} trend spanned a combined duration of {total_time} seconds\",\n",
        "        \"In summary, the data's {trend_type} behavior lasted for an accumulated time of {total_time} seconds\",\n",
        "        \"Recapitulating, the data's {trend_type} tendency endured for an aggregate timeframe of {total_time} seconds\",\n",
        "        \"The investigation concludes that the data's {trend_type} trend had a total lifespan of {total_time} seconds\",\n",
        "        \"To epitomize, the data's {trend_type} characteristic persevered for a sum of {total_time} seconds\",\n",
        "        \"Encapsulating the outcomes, the data's {trend_type} trend stretched across a total time of {total_time} seconds\",\n",
        "        \"In a nutshell, the data's {trend_type} propensity persisted for an accumulated duration of {total_time} seconds\",\n",
        "        \"Summarizing the results, the data's {trend_type} tendency spanned a total timeframe of {total_time} seconds\",\n",
        "        \"The examination reveals that the data's {trend_type} inclination endured for an aggregate of {total_time} seconds\",\n",
        "        \"To encapsulate the findings, the data's {trend_type} behavior lasted for a cumulative period of {total_time} seconds\",\n",
        "        \"In essence, the data exhibited a {trend_type} pattern for a combined time of {total_time} seconds\",\n",
        "        \"The analysis concludes that the data's {trend_type} trend had a total lifespan of {total_time} seconds\",\n",
        "        \"In summary, the data displayed a {trend_type} behavior for an aggregate time of {total_time} seconds\",\n",
        "        \"Overall, the data showed a {trend_type} trend over {total_time} seconds\",\n",
        "        \"In summary, a {trend_type} trend was observed across the span of {total_time} seconds\",\n",
        "        \"To conclude, the trend was {trend_type} over a period of {total_time} seconds\",\n",
        "        \"Summarizing, there was a {trend_type} trend throughout {total_time} seconds\",\n",
        "        \"Briefly, the data trended {trend_type} over the duration of {total_time} seconds\",\n",
        "        \"In total, the data showed a {trend_type} trend lasting {total_time} seconds\",\n",
        "        \"Concisely, the trend observed was {trend_type} for {total_time} seconds\",\n",
        "        \"The input data exhibited a {trend_type} trend during the {total_time} second period\",\n",
        "        \"Upon review, the data's trend was {trend_type} throughout the {total_time} seconds\",\n",
        "        \"The analysis highlighted a {trend_type} trend over the span of {total_time} seconds\",\n",
        "        \"Summarily, a {trend_type} direction was evident across {total_time} seconds of data\"\n",
        "    ],\n",
        "    \"gen_summary_4\": [\n",
        "        \"a {trend_type} pattern for {total_time} seconds\",\n",
        "        \"a {trend_type} trend for {total_time} seconds\",\n",
        "        \"a {trend_type} pattern for a total of {total_time} seconds\",\n",
        "        \"a {trend_type} trend for a total of {total_time} seconds\",\n",
        "        \"a {trend_type} pattern for a sum of {total_time} seconds\",\n",
        "        \"a {trend_type} trend for a sum of {total_time} seconds\",\n",
        "        \"a {trend_type} pattern for a cumulative period of {total_time} seconds\",\n",
        "        \"a {trend_type} trend for a cumulative period of {total_time} seconds\",\n",
        "        \"a {trend_type} pattern for an accumulated time of {total_time} seconds\",\n",
        "        \"a {trend_type} trend for an accumulated time of {total_time} seconds\",\n",
        "        \"a {trend_type} pattern for an aggregate time of {total_time} seconds\",\n",
        "        \"a {trend_type} trend for an aggregate time of {total_time} seconds\",\n",
        "        \"a {trend_type} pattern for {total_time} seconds in total\",\n",
        "        \"a {trend_type} trend for {total_time} seconds in total\",\n",
        "        \"a pattern of {trend_type} for {total_time} seconds\",\n",
        "        \"a trend of {trend_type} for {total_time} seconds\",\n",
        "        \"a {trend_type} trend observed over {total_time} seconds\",\n",
        "        \"a {trend_type} pattern observed over {total_time} seconds\",\n",
        "        \"a {trend_type} trend within a span of {total_time} seconds\",\n",
        "        \"a {trend_type} pattern within a span of {total_time} seconds\",\n",
        "        \"a sequence of {trend_type} occurring over {total_time} seconds\"\n",
        "    ],\n",
        "    \"gen_summary_6\": [\n",
        "        \"The overall trend is {overall_trend}.\",\n",
        "        \"The general trend observed is {overall_trend}.\",\n",
        "        \"Overall, the trend is {overall_trend}.\",\n",
        "        \"The primary trend detected is {overall_trend}.\",\n",
        "        \"In summary, the overall trend is {overall_trend}.\",\n",
        "        \"The main direction we're seeing is {overall_trend}.\",\n",
        "        \"The overarching trend is identified as {overall_trend}.\",\n",
        "        \"Key observation: the overall trend is {overall_trend}.\",\n",
        "        \"The general trend shows {overall_trend}.\",\n",
        "        \"According to the analysis, the overall trend is {overall_trend}.\",\n",
        "        \"The data reveals a {overall_trend} trend in general.\",\n",
        "        \"The predominant trend is observed to be {overall_trend}.\",\n",
        "        \"The overarching trend is determined to be {overall_trend}.\",\n",
        "        \"After calculation, the primary trend is identified as {overall_trend}.\",\n",
        "        \"The general trend is {overall_trend}.\",\n",
        "        \"The prevailing trend is {overall_trend}.\",\n",
        "        \"The trend overall is {overall_trend}.\",\n",
        "        \"The dominant trend is {overall_trend}.\",\n",
        "        \"In summary, the trend is {overall_trend}.\",\n",
        "        \"Broadly, the movement is {overall_trend}.\",\n",
        "        \"The main direction is {overall_trend}.\",\n",
        "        \"The overarching trend is characterized as {overall_trend}.\",\n",
        "        \"The trend direction is {overall_trend}.\",\n",
        "        \"Looking at the big picture, the trend is {overall_trend}.\",\n",
        "        \"Trend overview: {overall_trend}.\"\n",
        "    ],\n",
        "    \"gen_trend_q\": [\n",
        "        \"Kindly provide a detailed analysis of the trend changes observed in the {data}.\",\n",
        "        \"Please offer a comprehensive description of how the trends in the {data} have evolved.\",\n",
        "        \"I would appreciate a thorough explanation of the trend fluctuations that occurred within the {data}.\",\n",
        "        \"Could you please examine the {data} in depth and explain the trend shifts observed step by step?\",\n",
        "        \"I kindly request a detailed analysis of the trend changes present in the {data}.\",\n",
        "        \"Please evaluate the {data} trends and provide a detailed description of their development.\",\n",
        "        \"I would be grateful if you could offer a comprehensive account of the trend alterations within the {data}.\",\n",
        "        \"Could you kindly assess the {data} and provide a description of the trend transformations that took place step by step?\",\n",
        "        \"Could you analyze the trends observed in the {data} over the specified period step by step?\",\n",
        "        \"I’m interested in understanding how the {data} has evolved. Could you break down the trend changes for me in detail?\",\n",
        "        \"Please explore the {data} for me, highlighting any significant trends and changes that have occurred.\",\n",
        "        \"I’d appreciate a comprehensive overview of the trends within the {data}, with particular attention to any notable shifts or changes.\",\n",
        "        \"Can you dissect the {data} and explain the trend changes in a detailed manner?\",\n",
        "        \"I'm looking for an in-depth examination of the {data}. Could you elucidate the trends and pivotal changes?\",\n",
        "        \"Please conduct a thorough analysis of the {data}, focusing on the evolution of trends over time.\",\n",
        "        \"Could you delve into the {data} and provide a detailed synopsis of the trends and alterations observed?\",\n",
        "        \"Please analyze the trend shifts in the {data}.\",\n",
        "        \"I need an overview of changes in {data} trends.\",\n",
        "        \"Can you summarize trend fluctuations in the {data}?\",\n",
        "        \"Offer insights into the trend alterations within the {data}.\",\n",
        "        \"Detail the {data}'s trend transitions.\",\n",
        "        \"Examine the evolution of trends in the {data}.\",\n",
        "        \"Explore the changes in trends for the {data}.\",\n",
        "        \"Give a brief analysis of {data} trend developments.\",\n",
        "        \"Please analyze the trend changes in the {data}.\",\n",
        "        \"Describe the trend shifts observed in the {data}.\",\n",
        "        \"Examine how trends in the {data} have evolved.\",\n",
        "        \"What trend changes can be seen in the {data}?\",\n",
        "        \"Identify the trend variations within the {data}.\",\n",
        "        \"Explain the trend developments in the {data}.\",\n",
        "        \"Provide an overview of the trend patterns in the {data}.\",\n",
        "        \"Detail the significant trend modifications in the {data}.\",\n",
        "        \"Analyze the main trend alterations observed in the {data}.\"\n",
        "    ],\n",
        "    \"gen_trend_1\": [\n",
        "        \"Between {start_time} and {end_time} seconds, the data exhibited a {trend} trend.\",\n",
        "        \"The data showed a {trend} trend from {start_time} to {end_time} seconds.\",\n",
        "        \"A {trend} trend was observed in the data spanning {start_time} to {end_time} seconds.\",\n",
        "        \"The time period from {start_time} to {end_time} seconds was characterized by a {trend} trend in the data.\",\n",
        "        \"Over the course of {start_time} to {end_time} seconds, the data displayed a {trend} trend.\",\n",
        "        \"The data followed a {trend} trend during the time frame of {start_time} to {end_time} seconds.\",\n",
        "        \"From {start_time}s to {end_time}s, a {trend} trend was evident in the data.\",\n",
        "        \"The data manifested a {trend} trend within the {start_time} to {end_time} second range.\",\n",
        "        \"Throughout the {start_time} to {end_time} second interval, the data demonstrated a {trend} trend.\",\n",
        "        \"Data analysis from {start_time}s to {end_time}s indicated a {trend} trend.\",\n",
        "        \"In the timeframe from {start_time}s to {end_time}s, the data presented a {trend} trend.\",\n",
        "        \"Observing the data between {start_time} and {end_time} seconds revealed a {trend} trend.\",\n",
        "        \"The data, from {start_time}s to {end_time}s, revealed a {trend} trend.\",\n",
        "        \"From {start_time} to {end_time} seconds, there's a {trend} trend indicated by the data.\",\n",
        "        \"During the period from {start_time}s to {end_time}s, the data exhibited a {trend} trend.\",\n",
        "        \"Between {start_time} and {end_time} seconds, we observed a {trend} trend in the data.\",\n",
        "        \"The data from {start_time} to {end_time} seconds showed a {trend} trend.\",\n",
        "        \"A {trend} trend was evident in the data spanning from {start_time}s to {end_time}s.\",\n",
        "        \"The data displayed a {trend} trend during the period between {start_time}s and {end_time}s.\",\n",
        "        \"Within the {start_time} to {end_time} second range, the data presented a {trend} trend.\",\n",
        "        \"The data revealed a {trend} trend spanning from {start_time} to {end_time} seconds.\",\n",
        "        \"The data showcased a {trend} trend within the timeframe of {start_time}s to {end_time}s.\",\n",
        "        \"Between {start_time} and {end_time} seconds, the data showed a {trend} trend.\",\n",
        "        \"From {start_time} to {end_time} seconds, there was a {trend} trend observed in the data.\",\n",
        "        \"The data demonstrated a {trend} trend from {start_time}s to {end_time}s.\",\n",
        "        \"A {trend} trend in the data was evident between {start_time} and {end_time} seconds.\",\n",
        "        \"Observations from {start_time} to {end_time} seconds indicated a {trend} trend in the data.\",\n",
        "        \"Data between {start_time} seconds and {end_time} seconds revealed a {trend} trend.\",\n",
        "        \"From {start_time}s to {end_time}s, the trend in the data was {trend}.\",\n",
        "        \"The period from {start_time} to {end_time} seconds showed a {trend} trend in the data.\"\n",
        "    ],\n",
        "    \"gen_trend_2\": [\n",
        "        \"Subsequently, a {trend} trend was observed until {end_time}s.\",\n",
        "        \"Following this, the data showed a {trend} trend lasting up to {end_time} seconds.\",\n",
        "        \"Afterward, a {trend} trend emerged and continued until {end_time}s.\",\n",
        "        \"The previous trend was succeeded by a {trend} trend, which persisted up to {end_time} seconds.\",\n",
        "        \"A {trend} trend then followed, extending to {end_time}s.\",\n",
        "        \"The data then exhibited a {trend} trend, which carried on until {end_time} seconds.\",\n",
        "        \"After the previous trend, a {trend} trend was noted, lasting until {end_time}s.\",\n",
        "        \"The subsequent trend was {trend}, and it remained until {end_time} seconds.\",\n",
        "        \"The {trend} trend continued until {end_time} seconds.\",\n",
        "        \"The data maintained a {trend} trend through {end_time}s.\",\n",
        "        \"Up to {end_time} seconds, the data persisted in a {trend} trend.\",\n",
        "        \"The {trend} trend carried on till {end_time} seconds.\",\n",
        "        \"A {trend} trend was sustained by the data up to {end_time} seconds.\",\n",
        "        \"The data upheld a {trend} trend leading up to {end_time}s.\",\n",
        "        \"Until {end_time}s, the data prolonged its {trend} trend.\",\n",
        "        \"The {trend} trend in the data endured up until {end_time} seconds.\",\n",
        "        \"The data perpetuated a {trend} trend all the way to {end_time}s.\",\n",
        "        \"Extending to {end_time}s, the data maintained its {trend} trend.\",\n",
        "        \"This was succeeded by a {trend} trend until {end_time} seconds.\",\n",
        "        \"Subsequently, a {trend} trend was observed up to {end_time} seconds.\",\n",
        "        \"Following this, the data exhibited a {trend} trend up to {end_time}s.\",\n",
        "        \"Thereafter, a {trend} trend continued until {end_time} seconds.\",\n",
        "        \"Afterwards, the trend shifted to {trend} until {end_time} seconds.\",\n",
        "        \"The situation then transitioned into a {trend} trend up to {end_time} seconds.\",\n",
        "        \"Subsequently, the trend moved to {trend} lasting until {end_time} seconds.\",\n",
        "        \"This period was marked by a {trend} trend up until {end_time} seconds.\",\n",
        "        \"Following that period, a {trend} trend was evident until {end_time}s.\",\n",
        "        \"The data then exhibit a {trend} trend until reaching {end_time}s.\",\n",
        "        \"The trend then shifted to a {trend} direction, lasting until {end_time}s.\",\n",
        "        \"What followed was a {trend} trend, extending to {end_time} seconds.\",\n",
        "        \"The data then entered a {trend} phase, which lasted until {end_time} seconds.\",\n",
        "        \"Next, the trend took a {trend} turn, continuing up to {end_time}s.\",\n",
        "        \"This phase was characterized by a {trend} trend until {end_time} seconds.\",\n",
        "        \"It was then that the trend veered towards {trend}, which persisted until {end_time}s.\",\n",
        "        \"Subsequently, the {trend} trend became apparent, prevailing until {end_time} seconds.\",\n",
        "        \"The trend subsequently morphed into a {trend} pattern, holding until {end_time} seconds.\",\n",
        "        \"Following this phase, the trend evolved into a {trend} trajectory until {end_time} seconds.\",\n",
        "        \"Thereafter, the sequence of events led to a {trend} trend, which concluded at {end_time}s.\",\n",
        "        \"Continuing onwards, a {trend} trend was observed through to {end_time} seconds.\",\n",
        "        \"The trend subsequently evolved into a {trend} mode, prevailing up until {end_time} seconds.\",\n",
        "        \"After that, the {trend} trend became the dominant pattern until {end_time} seconds.\",\n",
        "        \"The pattern then entered a {trend} phase, which sustained up to {end_time}s.\",\n",
        "        \"Following this development, the trend solidified into a {trend} direction, continuing until {end_time} seconds.\",\n",
        "        \"The data then aligned with a {trend} trend, which was maintained up to {end_time} seconds.\",\n",
        "        \"Subsequent observations indicated a {trend} trend, lasting until {end_time}s.\",\n",
        "        \"The period following showed a sustained {trend} trend up to {end_time} seconds.\",\n",
        "        \"The subsequent phase was defined by a {trend} trend, enduring until {end_time}s.\",\n",
        "        \"The trend then progressed to a {trend} state, concluding at {end_time} seconds.\",\n",
        "        \"Following this interval, the trend gravitated towards {trend}, persisting through {end_time} seconds.\",\n",
        "        \"Subsequently, the trend shifted into a {trend} trend, which lasted till {end_time} seconds.\",\n",
        "        \"In the next phase, a clear {trend} trend was evident, continuing right up to {end_time}s.\",\n",
        "        \"The data's trajectory shifted towards a {trend} trend, lasting up until {end_time}s.\",\n",
        "        \"After these developments, the {trend} trend took hold, extending to {end_time} seconds.\",\n",
        "        \"The sequence of events led to a {trend} trend, which remained until {end_time}s.\",\n",
        "        \"Then, a {trend} trend until {end_time}s.\",\n",
        "        \"A {trend} trend followed, through {end_time} seconds.\",\n",
        "        \"{trend} trend up to {end_time} seconds.\",\n",
        "        \"Next, {trend} until {end_time}s.\",\n",
        "        \"Followed by {trend} to {end_time}s.\",\n",
        "        \"{trend} persisted until {end_time} seconds.\",\n",
        "        \"Then, {trend} through {end_time} seconds.\",\n",
        "        \"Subsequently, {trend} till {end_time}s.\",\n",
        "        \"{trend} until {end_time} seconds.\",\n",
        "        \"Afterward, {trend} to {end_time}s.\",\n",
        "        \"Continues {trend} until {end_time} seconds.\",\n",
        "        \"Trended {trend} until {end_time}s.\",\n",
        "        \"Showed {trend} until {end_time} seconds.\",\n",
        "        \"Until {end_time} seconds, we observed a {trend} trend.\",\n",
        "        \"As of {end_time}s, the trend was {trend}.\",\n",
        "        \"By {end_time} seconds, there was a noticeable {trend} trend.\",\n",
        "        \"By the time it reached {end_time}s, a {trend} trend was evident.\"\n",
        "    ],\n",
        "    \"gen_trend_3\": [\n",
        "        \"Ultimately, a {trend} trend was seen in the data up until {end_time} seconds.\",\n",
        "        \"The data concluded with a {trend} trend, lasting up to {end_time} seconds.\",\n",
        "        \"The final trend observed in the data was {trend}, which continued up to {end_time} seconds.\",\n",
        "        \"In the end, the data displayed a {trend} trend that lasted up to {end_time}s.\",\n",
        "        \"The concluding trend in the data was {trend}, which persevered up until {end_time}s.\",\n",
        "        \"Up to {end_time}s, the data finished with a {trend} trend.\",\n",
        "        \"The data's final trend was {trend}, which was maintained up to {end_time}s.\",\n",
        "        \"Lastly, a {trend} trend was noted in the data, enduring until {end_time} seconds.\",\n",
        "        \"The data's concluding trend was {trend}, which persisted up to {end_time} seconds.\",\n",
        "        \"At the end, the data exhibited a {trend} trend that carried on until {end_time}s.\",\n",
        "        \"The terminal trend in the data was {trend}, which held fast up to {end_time} seconds.\",\n",
        "        \"In conclusion, the data demonstrated a {trend} trend that continued up to {end_time} seconds.\",\n",
        "        \"Up until {end_time} seconds, the final trend {trend} was noted.\",\n",
        "        \"The final trend had been {trend} up to {end_time}s.\",\n",
        "        \"Continuing until {end_time}s, the trend was decidedly {trend}.\",\n",
        "        \"Ultimately, by {end_time} seconds, a {trend} trend had been observed.\",\n",
        "        \"Conclusively, up to {end_time}s, the data showed a {trend} trend.\",\n",
        "        \"In the end, until {end_time}s, there was an observable {trend} trend.\",\n",
        "        \"To conclude, by {end_time}s, a {trend} trend was noted in the data.\",\n",
        "        \"The observation concluded with a {trend} trend by {end_time}s.\",\n",
        "        \"Finishing at {end_time} seconds, the data revealed a {trend} trend.\",\n",
        "        \"The concluding observation in the data was a {trend} trend, persisting up to {end_time} seconds.\",\n",
        "        \"The data's final chapter was characterized by a {trend} trend, lasting up to {end_time}s.\",\n",
        "        \"The data's end was marked by a {trend} trend, which sustained its direction up to {end_time}s.\",\n",
        "        \"The final data trend, {trend}, stayed its course to {end_time} seconds.\",\n",
        "        \"The data's final trend, {trend}, persisted up to {end_time} seconds.\",\n",
        "        \"The data ended with a {trend} trend up to {end_time}s.\",\n",
        "        \"The data's end saw a {trend} trend, enduring to {end_time} seconds.\",\n",
        "        \"The data's end was {trend}, a trend that held to {end_time} seconds.\"\n",
        "    ],\n",
        "    \"gen_trend_4\": [\n",
        "        \"In summary, the data contains a total of {upward_num} segments with continuous {upward_trend} trends\",\n",
        "        \"Overall, the data shows {upward_num} {upward_trend} trends\",\n",
        "        \"The data reveals a total of {upward_num} segments exhibiting continuous {upward_trend} trends\",\n",
        "        \"Summarizing the data, there are {upward_num} ongoing {upward_trend} trends\",\n",
        "        \"In essence, the dataset comprises {upward_num} segments with persistent {upward_trend} trends\",\n",
        "        \"The summary indicates that the data includes {upward_num} segments with {upward_trend} trends\",\n",
        "        \"In brief, the dataset shows {upward_num} segments with {upward_trend} trends\",\n",
        "        \"To sum up, the data contains {upward_num} uninterrupted {upward_trend} trends\",\n",
        "        \"The analysis indicates that there are {upward_num} segments with continuous {upward_trend} trends\",\n",
        "        \"In short, the data reveals {upward_num} segments with {upward_trend} trends\",\n",
        "        \"According to the data, there are {upward_num} segments showing continuous {upward_trend} trends\",\n",
        "        \"In summary, the analysis found {upward_num} {upward_trend} trends\",\n",
        "        \"The summary reveals {upward_num} segments with {upward_trend} trends\"\n",
        "    ],\n",
        "    \"gen_trend_5\": [\n",
        "        \"{downward_num} segments with continuous {downward_trend} trends\",\n",
        "        \"{downward_num} {downward_trend} trends\",\n",
        "        \"{downward_num} segments exhibiting continuous {downward_trend} trends\",\n",
        "        \"{downward_num} ongoing {downward_trend} trends\",\n",
        "        \"{downward_num} segments with persistent {downward_trend} trends\",\n",
        "        \"{downward_num} segments with {downward_trend} trends\",\n",
        "        \"{downward_num} segments with {downward_trend} trends\",\n",
        "        \"{downward_num} uninterrupted {downward_trend} trends\",\n",
        "        \"{downward_num} segments with continuous {downward_trend} trends\",\n",
        "        \"{downward_num} segments with {downward_trend} trends\",\n",
        "        \"{downward_num} segments showing continuous {downward_trend} trends\",\n",
        "        \"{downward_num} {downward_trend} trends\",\n",
        "        \"{downward_num} segments with {downward_trend} trends\"\n",
        "    ],\n",
        "    \"gen_trend_6\": [\n",
        "        \"and {stable_num} segments with {stable_trend} trends\",\n",
        "        \"and {stable_num} {stable_trend} trends\",\n",
        "        \"and {stable_num} segments exhibiting {stable_trend} trends\",\n",
        "        \"and {stable_num} {stable_trend} trends\",\n",
        "        \"and {stable_num} segments with {stable_trend} trends\",\n",
        "        \"and {stable_num} segments with {stable_trend} trends\",\n",
        "        \"and {stable_num} segments with {stable_trend} trends\",\n",
        "        \"and {stable_num} uninterrupted {stable_trend} trends\",\n",
        "        \"and {stable_num} segments with continuous {stable_trend} trends\",\n",
        "        \"and {stable_num} segments with {stable_trend} trends\",\n",
        "        \"and {stable_num} segments showing {stable_trend} trends\",\n",
        "        \"and {stable_num} {stable_trend} trends\",\n",
        "        \"and {stable_num} segments with {stable_trend} trends\"\n",
        "    ],\n",
        "    \"gen_subtrend_q\": [\n",
        "        \"Please describe how the input {data}'s trends changed from {start_time}s to {end_time}s.\",\n",
        "        \"Kindly analyze the {data} trend variations between {start_time} and {end_time} seconds.\",\n",
        "        \"Please provide an overview of how the {data} trends evolved from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"I would appreciate if you could describe the {data} trend fluctuations that occurred within the {start_time} to {end_time} second timeframe.\",\n",
        "        \"Could you please examine the {data} and explain the trend shifts observed from {start_time}s until {end_time}s?\",\n",
        "        \"I kindly request an analysis of the {data} trend changes spanning the period between {start_time} and {end_time} seconds.\",\n",
        "        \"Please evaluate the {data} trends and describe how they developed from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"I would be grateful if you could provide a description of the {data} trend alterations that took place from {start_time}s to {end_time}s.\",\n",
        "        \"Could you kindly assess the {data} trend transformations occurring within the {start_time} to {end_time} second range?\",\n",
        "        \"Please analyze the trend of the {data} from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"Could you describe how the {data} trend changes between {start_time}s and {end_time}s?\",\n",
        "        \"I'm interested in the input {data}'s trend from {start_time} to {end_time} seconds. Can you break it down for me?\",\n",
        "        \"Can you provide an analysis of the {data} trend from {start_time} seconds to {end_time} seconds?\",\n",
        "        \"Would you be able to detail the trend changes in the given {data} from {start_time} to {end_time} seconds?\",\n",
        "        \"I'd like an overview of how the input {data} evolves between {start_time} and {end_time} seconds. Can you help?\",\n",
        "        \"Please give me an insight into the {data}'s progression from {start_time}s to {end_time}s.\",\n",
        "        \"Could you examine the shift in {data} trends from {start_time} seconds to {end_time} seconds?\",\n",
        "        \"I'm looking for a summary of the {data} trends between {start_time} and {end_time} seconds. What can you tell me?\",\n",
        "        \"Can you explore the changes in the {data}'s trend from {start_time} to {end_time} seconds?\",\n",
        "        \"Describe the trend of {data} from {start_time}s to {end_time}s.\",\n",
        "        \"How did the {data} trend evolve from {start_time} to {end_time} seconds?\",\n",
        "        \"Please outline the change in {data}'s trend between {start_time}s and {end_time}s.\",\n",
        "        \"Can you detail the shift in {data} from {start_time} seconds to {end_time} seconds?\",\n",
        "        \"Explain the transition in {data} from {start_time} to {end_time} seconds.\",\n",
        "        \"Describe the {data}'s trend changes from {start_time} seconds to {end_time} seconds.\",\n",
        "        \"How did the {data}'s trends evolve between {start_time} and {end_time} seconds?\",\n",
        "        \"Explain the {data}'s trend shifts from {start_time}s to {end_time}s.\",\n",
        "        \"Analyze the {data}'s trend variations between {start_time}s and {end_time}s.\",\n",
        "        \"What were the {data}'s trend modifications from {start_time} to {end_time} seconds?\",\n",
        "        \"Summarize the {data}'s trend developments between {start_time} and {end_time} seconds.\",\n",
        "        \"Detail the {data}'s trend fluctuations from {start_time} to {end_time} seconds.\",\n",
        "        \"Examine the {data}'s trend patterns between {start_time}s and {end_time}s.\",\n",
        "        \"Describe how the {data}'s trends altered from {start_time} to {end_time} seconds.\",\n",
        "        \"Provide an overview of the {data}'s trend changes between {start_time} seconds and {end_time} seconds.\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "def capitalize_first_letter(string):\n",
        "    if len(string) == 0:\n",
        "        return string\n",
        "    else:\n",
        "        return string[0].upper() + string[1:]\n",
        "\n",
        "\n",
        "def check_a_an(sentence):\n",
        "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
        "    vowels = 'aeiouAEIOU'\n",
        "    corrected_sentence = sentence\n",
        "\n",
        "    for i in range(len(words)):\n",
        "        if words[i] in ['a', 'an', 'A', 'An']:\n",
        "            if i + 1 < len(words):\n",
        "                next_word = words[i + 1]\n",
        "                if words[i] == 'a' and next_word[0] in vowels:\n",
        "                    corrected_sentence = corrected_sentence.replace(f' a {next_word}', f' an {next_word}', 1)\n",
        "                elif words[i] == 'A' and next_word[0] in vowels:\n",
        "                    corrected_sentence = corrected_sentence.replace(f' A {next_word}', f' An {next_word}', 1)\n",
        "                elif words[i] == 'an' and next_word[0] not in vowels:\n",
        "                    corrected_sentence = corrected_sentence.replace(f' an {next_word}', f' a {next_word}', 1)\n",
        "                elif words[i] == 'An' and next_word[0] not in vowels:\n",
        "                    corrected_sentence = corrected_sentence.replace(f' An {next_word}', f' A {next_word}', 1)\n",
        "\n",
        "    return corrected_sentence\n",
        "\n",
        "def analyze_trend(time_series, sample_rate, start_point=0):\n",
        "    # Calculate the time interval between data points\n",
        "    time_interval = 1 / sample_rate\n",
        "\n",
        "    # Initialize lists to store the analysis results\n",
        "    from_time, to_time, from_value, to_value, trend = [], [], [], [], []\n",
        "\n",
        "    # Analyze the trend between consecutive data points\n",
        "    for i in range(len(time_series) - 1):\n",
        "        start_time = round((start_point + i) * time_interval, 2)\n",
        "        end_time = round((start_point + i + 1) * time_interval, 2)\n",
        "        start_val = time_series[i]\n",
        "        end_val = time_series[i + 1]\n",
        "\n",
        "        # Determine the trend\n",
        "        if start_val == end_val:\n",
        "            trend_type = 'steady'\n",
        "        elif start_val < end_val:\n",
        "            trend_type = 'increase'\n",
        "        else:\n",
        "            trend_type = 'decrease'\n",
        "\n",
        "        # Append the results to the lists\n",
        "        from_time.append(start_time)\n",
        "        to_time.append(end_time)\n",
        "        from_value.append(start_val)\n",
        "        to_value.append(end_val)\n",
        "        trend.append(trend_type)\n",
        "\n",
        "    # Create a DataFrame from the results\n",
        "    result_df = pd.DataFrame({\n",
        "        'from_time': from_time,\n",
        "        'to_time': to_time,\n",
        "        'from_value': from_value,\n",
        "        'to_value': to_value,\n",
        "        'trend': trend\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def merge_adjacent_rows(df):\n",
        "    # List to store the merged rows\n",
        "    merged_rows = []\n",
        "\n",
        "    # Variables to store the start of the current segment\n",
        "    current_start_time = df.iloc[0]['from_time']\n",
        "    current_start_value = df.iloc[0]['from_value']\n",
        "    current_trend = df.iloc[0]['trend']\n",
        "    current_values = [current_start_value]\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        if row['trend'] == current_trend:\n",
        "            # Continue accumulating values\n",
        "            current_values.append(row['to_value'])\n",
        "        else:\n",
        "            # Close the current segment and start a new one\n",
        "            merged_rows.append({\n",
        "                'start_time': current_start_time,\n",
        "                'end_time': df.iloc[index - 1]['to_time'],\n",
        "                'start_value': current_start_value,\n",
        "                'end_value': df.iloc[index - 1]['to_value'],\n",
        "                'trend': current_trend,\n",
        "                'values': current_values.copy()\n",
        "            })\n",
        "            current_start_time = row['from_time']\n",
        "            current_start_value = row['from_value']\n",
        "            current_trend = row['trend']\n",
        "            current_values = [current_start_value, row['to_value']]\n",
        "\n",
        "    # Append the last segment\n",
        "    merged_rows.append({\n",
        "        'start_time': current_start_time,\n",
        "        'end_time': df.iloc[-1]['to_time'],\n",
        "        'start_value': current_start_value,\n",
        "        'end_value': df.iloc[-1]['to_value'],\n",
        "        'trend': current_trend,\n",
        "        'values': current_values\n",
        "    })\n",
        "\n",
        "    # Create a DataFrame from the merged rows\n",
        "    merged_df = pd.DataFrame(merged_rows)\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "\n",
        "def df2mkd(df):\n",
        "    header = \"| \" + \" | \".join(df.columns) + \" |\"\n",
        "    separator = \"|---\" * len(df.columns) + \"|\"\n",
        "\n",
        "    rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        row_str = \"| \" + \" | \".join(str(value) for value in row) + \" |\"\n",
        "        rows.append(row_str)\n",
        "\n",
        "    markdown_table = \"\\n\".join([header, separator] + rows)\n",
        "    return markdown_table\n",
        "\n",
        "\n",
        "def calculate_total_time(df):\n",
        "    \"\"\"\n",
        "    Calculate the total duration for each trend in the dataframe.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): A DataFrame with columns: from_time, to_time, trend.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: A DataFrame with columns: trend, total_time.\n",
        "    \"\"\"\n",
        "    # Group by the trend and sum the duration for each trend\n",
        "    total_time_by_trend = df.groupby('trend').apply(\n",
        "        lambda x: round((x['end_time'] - x['start_time']).sum(), 2)).reset_index(\n",
        "        name='total_time')\n",
        "\n",
        "    return total_time_by_trend\n",
        "\n",
        "\n",
        "def num_to_words(num):\n",
        "    units = ['', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "    teens = ['ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen', 'seventeen', 'eighteen',\n",
        "             'nineteen']\n",
        "    tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']\n",
        "    scales = ['', 'thousand', 'million', 'billion']\n",
        "\n",
        "    if num < 0:\n",
        "        return \"minus \" + num_to_words(abs(num))\n",
        "\n",
        "    if num < 10:\n",
        "        return units[int(num)]\n",
        "\n",
        "    if num < 20:\n",
        "        return teens[int(num) - 10]\n",
        "\n",
        "    if num < 100:\n",
        "        return tens[int(num) // 10] + (\" \" + num_to_words(num % 10) if num % 10 != 0 else \"\")\n",
        "\n",
        "    if num < 1000:\n",
        "        return units[int(num) // 100] + \" hundred\" + (\" \" + num_to_words(num % 100) if num % 100 != 0 else \"\")\n",
        "\n",
        "    for i, scale in enumerate(scales[1:], 1):\n",
        "        if num < 1000 ** (i + 1):\n",
        "            return num_to_words(num // (1000 ** i)) + \" \" + scale + (\n",
        "                \" \" + num_to_words(num % (1000 ** i)) if num % (1000 ** i) != 0 else \"\")\n",
        "\n",
        "\n",
        "def convert_number(num):\n",
        "    if '.' in str(num):\n",
        "        whole, decimal = str(num).split('.')\n",
        "        if decimal == '0':\n",
        "            return num_to_words(int(num))\n",
        "        else:\n",
        "            return num_to_words(int(whole)) + \" point \" + \" \".join([num_to_words(int(digit)) for digit in decimal])\n",
        "    else:\n",
        "        return num_to_words(int(num))\n",
        "\n",
        "\n",
        "def format_floart_2_int(num):\n",
        "    if isinstance(num, float) and num.is_integer():\n",
        "        return int(num)\n",
        "    else:\n",
        "        return num\n",
        "\n",
        "\n",
        "def select_random_pair():\n",
        "    word_pairs = PROMPT_DICT[\"trend_synonyms\"]\n",
        "    upward_word = random.choice(list(word_pairs.keys()))\n",
        "    downward_word = word_pairs[upward_word]\n",
        "    steady_word = random.choice(PROMPT_DICT[\"steady_synonyms\"])\n",
        "    return [upward_word, downward_word, steady_word]\n",
        "\n",
        "\n",
        "def choose_word(input_trend, pair):\n",
        "    if input_trend == \"steady\":\n",
        "        return pair[2]\n",
        "    elif input_trend == \"increase\" or input_trend == \"upward\":\n",
        "        return pair[0]\n",
        "    else:\n",
        "        return pair[1]\n",
        "\n",
        "\n",
        "def generate_smry_text(reading, data_df, sensor_type, pair_list):\n",
        "    \"\"\"\n",
        "    Generate a text description of the data.\n",
        "\n",
        "    Parameters:\n",
        "    - data_df (DataFrame): A DataFrame with columns: from_time, to_time, from_value, to_value, trend, values.\n",
        "    - total_time_df (DataFrame): A DataFrame with columns: trend, total_time.\n",
        "\n",
        "    Returns:\n",
        "    - str: A text description of the data.\n",
        "    \"\"\"\n",
        "    total_time_df = calculate_total_time(data_df)\n",
        "    total_time_mkd = df2mkd(total_time_df)\n",
        "    connect_words = [\"followed by \", \"came after \", \"and then \", \"trailed by \", \"which was followed by \",\n",
        "                     \"succeeded by \"]\n",
        "    # Initialize a list to store the text description\n",
        "    prompts_templates1 = PROMPT_DICT[\"gen_summary_1\"]\n",
        "    prompts_templates2 = PROMPT_DICT[\"gen_summary_2\"]\n",
        "    prompts_templates2_2 = PROMPT_DICT[\"gen_summary_2_2\"]\n",
        "    prompts_templates3 = PROMPT_DICT[\"gen_summary_3\"]\n",
        "    prompts_templates4 = PROMPT_DICT[\"gen_summary_4\"]\n",
        "\n",
        "    trend_num = len(total_time_df)\n",
        "    change_num = len(data_df)\n",
        "    data_types = [\"time series data\", \"sensor data\"]\n",
        "\n",
        "    selected_data_type = random.choice(data_types)\n",
        "\n",
        "    selected_template1 = random.choice(prompts_templates1)\n",
        "    selected_template2 = random.choice(prompts_templates2)\n",
        "    selected_template2_2 = random.choice(prompts_templates2_2)\n",
        "    selected_template3 = random.choice(prompts_templates3)\n",
        "    selected_template4 = random.choice(prompts_templates4)\n",
        "    selected_template5 = random.choice(prompts_templates4)\n",
        "\n",
        "    text = []\n",
        "    text.append(capitalize_first_letter(\n",
        "        selected_template1.format(data_name=selected_data_type, sensor_name=sensor_type,\n",
        "                                  start_time=data_df['start_time'].iloc[0], end_time=data_df['end_time'].iloc[-1],)))\n",
        "    if trend_num == 1:\n",
        "        text.append(capitalize_first_letter(\n",
        "            selected_template2_2.format(trend_num=random.choice([trend_num, convert_number(trend_num)]))))\n",
        "    else:\n",
        "        text.append(capitalize_first_letter(\n",
        "            selected_template2.format(trend_num=random.choice([trend_num, convert_number(trend_num)]),\n",
        "                                      change_num=random.choice([change_num, convert_number(change_num)]))))\n",
        "\n",
        "    i_t = 0\n",
        "    for index, t in total_time_df.iterrows():\n",
        "        if i_t == 0:\n",
        "            if len(total_time_df) == 1:\n",
        "                text.append(capitalize_first_letter(\n",
        "                    selected_template3.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
        "                                              total_time=f\"{t['total_time']:.2f}\")) + \".\")\n",
        "            else:\n",
        "                text.append(capitalize_first_letter(\n",
        "                    selected_template3.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
        "                                              total_time=f\"{t['total_time']:.2f}\")) + \",\")\n",
        "        elif i_t < len(total_time_df) - 1:\n",
        "            text.append(\n",
        "                random.choice(connect_words) + selected_template4.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
        "                                                                         total_time=f\"{t['total_time']:.2f}\") + \",\")\n",
        "        else:\n",
        "            text.append(\n",
        "                \"and \" + selected_template5.format(trend_type=choose_word(t[\"trend\"], pair_list),\n",
        "                                                   total_time=f\"{t['total_time']:.2f}\") + \".\")\n",
        "        i_t += 1\n",
        "\n",
        "    differences = np.diff(reading)\n",
        "    sum_of_differences = np.sum(differences)\n",
        "    if sum_of_differences > 0:\n",
        "        overall_trend = choose_word(\"upward\", pair_list)\n",
        "    elif sum_of_differences < 0:\n",
        "        overall_trend = choose_word(\"downward\", pair_list)\n",
        "    else:\n",
        "        overall_trend = choose_word(\"steady\", pair_list)\n",
        "\n",
        "    if change_num > 1:\n",
        "        prompts_templates7 = PROMPT_DICT[\"gen_summary_6\"]\n",
        "        selected_template7 = random.choice(prompts_templates7)\n",
        "\n",
        "        text.append(capitalize_first_letter(\n",
        "            selected_template7.format(overall_trend=overall_trend)))\n",
        "\n",
        "    return check_a_an(' '.join(text)), total_time_mkd\n",
        "\n",
        "\n",
        "def generate_trend_text(data_df, pair_list):\n",
        "    text_detailed = []\n",
        "\n",
        "    prompts_templates1 = PROMPT_DICT[\"gen_trend_1\"]\n",
        "    prompts_templates2 = PROMPT_DICT[\"gen_trend_2\"]\n",
        "    prompts_templates3 = PROMPT_DICT[\"gen_trend_3\"]\n",
        "\n",
        "    i_d = 0\n",
        "    for index, d in data_df.iterrows():\n",
        "        if i_d == 0:\n",
        "            selected_template1 = random.choice(prompts_templates1)\n",
        "            text_detailed.append(capitalize_first_letter(\n",
        "                selected_template1.format(start_time=d['start_time'],\n",
        "                                          end_time=d['end_time'],\n",
        "                                          trend=choose_word(d[\"trend\"],\n",
        "                                                            pair_list))))\n",
        "        elif i_d < len(data_df) - 1:\n",
        "            selected_template2 = random.choice(prompts_templates2)\n",
        "            text_detailed.append(capitalize_first_letter(\n",
        "                selected_template2.format(end_time=d['end_time'],\n",
        "                                          trend=choose_word(d[\"trend\"],\n",
        "                                                            pair_list))))\n",
        "        else:\n",
        "            selected_template3 = random.choice(prompts_templates3)\n",
        "            text_detailed.append(capitalize_first_letter(\n",
        "                selected_template3.format(end_time=d['end_time'],\n",
        "                                          trend=choose_word(d[\"trend\"],\n",
        "                                                            pair_list))))\n",
        "        i_d += 1\n",
        "\n",
        "    prompts_templates4 = PROMPT_DICT[\"gen_trend_4\"]\n",
        "    prompts_templates5 = PROMPT_DICT[\"gen_trend_5\"]\n",
        "    prompts_templates6 = PROMPT_DICT[\"gen_trend_6\"]\n",
        "\n",
        "    rdm_list = list(range(0, len(prompts_templates4)))\n",
        "    selected_num = random.choice(rdm_list)\n",
        "\n",
        "    selected_template4 = prompts_templates4[selected_num]\n",
        "    selected_template5 = prompts_templates5[selected_num]\n",
        "    selected_template6 = prompts_templates6[selected_num]\n",
        "\n",
        "    trend_counts = data_df['trend'].value_counts()\n",
        "    num_trends = len(trend_counts)\n",
        "\n",
        "    for i_n in range(num_trends):\n",
        "        if i_n == 0:\n",
        "            if num_trends > 1:\n",
        "                text_detailed.append(capitalize_first_letter(\n",
        "                    selected_template4.format(upward_num=trend_counts.values[i_n],\n",
        "                                              upward_trend=choose_word(trend_counts.index[i_n], pair_list))) + ',')\n",
        "        elif i_n < num_trends - 1:\n",
        "            text_detailed.append(\n",
        "                selected_template5.format(downward_num=trend_counts.values[i_n],\n",
        "                                          downward_trend=choose_word(trend_counts.index[i_n], pair_list)) + ',')\n",
        "        else:\n",
        "            text_detailed.append(\n",
        "                selected_template6.format(stable_num=trend_counts.values[i_n],\n",
        "                                          stable_trend=choose_word(trend_counts.index[i_n], pair_list)) + '.')\n",
        "\n",
        "    return check_a_an(' '.join(text_detailed))\n",
        "\n",
        "\n",
        "def generate_simple_trend_text(data_df, pair_list):\n",
        "    text_detailed = []\n",
        "\n",
        "    prompts_templates = [\n",
        "        \"{start_time}s to {end_time}s: {trend}\",\n",
        "        \"{start_time} seconds to {end_time} seconds: {trend}\",\n",
        "        \"{start_time} to {end_time} seconds: {trend}\",\n",
        "        \"{start_time}-{end_time} seconds: {trend}\",\n",
        "        \"{start_time}-{end_time}s: {trend}\",\n",
        "        \"{start_time}s-{end_time}s: {trend}\"\n",
        "    ]\n",
        "\n",
        "    prompts_templates_2 = [\n",
        "        \"Number of {trend} trends: {num}\",\n",
        "        \"Count of {trend} trends: {num}\",\n",
        "        \"Total {trend} trends: {num}\",\n",
        "        \"Number of {trend} segments: {num}\",\n",
        "        \"Count of {trend} segments: {num}\",\n",
        "        \"Total {trend} segments: {num}\"\n",
        "    ]\n",
        "\n",
        "    selected_template = random.choice(prompts_templates)\n",
        "    selected_template2 = random.choice(prompts_templates_2)\n",
        "\n",
        "    for index, df in data_df.iterrows():\n",
        "        text_detailed.append(selected_template.format(start_time=df['start_time'],\n",
        "                                                      end_time=df['end_time'],\n",
        "                                                      trend=choose_word(df[\"trend\"],\n",
        "                                                                        pair_list)))\n",
        "\n",
        "    trend_counts = data_df['trend'].value_counts()\n",
        "    num_trends = len(trend_counts)\n",
        "    if num_trends > 1:\n",
        "        for i_n in range(num_trends):\n",
        "            text_detailed.append(selected_template2.format(trend=choose_word(trend_counts.index[i_n], pair_list),\n",
        "                                                           num=trend_counts.values[i_n]))\n",
        "\n",
        "    return check_a_an('\\n'.join(text_detailed))\n",
        "\n",
        "\n",
        "def dscb_trend(df, sensor_type, pair_list, whether_gpt=False, model_type='3.5'):\n",
        "    data_types = [\"time series data\", \"sensor data\"]\n",
        "    prompts_templates = PROMPT_DICT[\"gen_subtrend_q\"]\n",
        "\n",
        "    selected_data_type = random.choice(data_types)\n",
        "    selected_template = random.choice(prompts_templates)\n",
        "\n",
        "    question = selected_template.format(data=selected_data_type,\n",
        "                                        start_time=df[\"start_time\"].iloc[0],\n",
        "                                        end_time=df['end_time'].iloc[-1]\n",
        "                                        )\n",
        "    answer = generate_trend_text(df, pair_list)\n",
        "\n",
        "    return {\n",
        "        \"Q\": question,\n",
        "        \"A\": answer,\n",
        "        \"type\": \"trend\"\n",
        "    }\n",
        "\n",
        "\n",
        "def dscb_simple_trend(df, sensor_type, pair_list, whether_gpt=False, model_type='3.5'):\n",
        "    data_types = [\"time series data\", \"sensor data\"]\n",
        "    prompts_templates = PROMPT_DICT[\"gen_trend_q\"]\n",
        "\n",
        "    selected_data_type = random.choice(data_types)\n",
        "    selected_template = random.choice(prompts_templates)\n",
        "\n",
        "    question = selected_template.format(data=selected_data_type)\n",
        "\n",
        "    answer = generate_simple_trend_text(df, pair_list)\n",
        "\n",
        "    return {\n",
        "        \"Q\": question,\n",
        "        \"A\": answer,\n",
        "        \"type\": \"simple_trend\"\n",
        "    }"
      ],
      "metadata": {
        "id": "wWrA5fwX3p0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'subject': 'subject1', 'activity_name': 'Standing still (1 min)', 'activity': np.int64(0), 'segments': [0, 61]}\n"
          ]
        }
      ],
      "source": [
        "print(all_test_labels[0])"
      ],
      "metadata": {
        "id": "5-9b2GK03p0G",
        "outputId": "af324beb-9c03-4744-b639-57658f6d381d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "source": [
        "def QA_summary(reading, trend_df, sensor_type, pair_list, whether_gpt=False, model_type='3.5'):\n",
        "    data_types = [\"time series data\", \"sensor data\"]\n",
        "    prompts_templates = PROMPT_DICT[\"gen_subtrend_q\"]\n",
        "\n",
        "    selected_data_type = random.choice(data_types)\n",
        "    selected_template = random.choice(prompts_templates)\n",
        "\n",
        "    question = selected_template.format(data=selected_data_type,\n",
        "                                        start_time=trend_df[\"start_time\"].iloc[0],\n",
        "                                        end_time=trend_df['end_time'].iloc[-1]\n",
        "                                        )\n",
        "\n",
        "    answer, smry_mkd_df = generate_smry_text(reading, trend_df, sensor_type, pair_list)\n",
        "    return {\n",
        "        \"Q\": question,\n",
        "        \"A\": answer,\n",
        "        \"smry_table\": smry_mkd_df,\n",
        "        \"type\": \"summary\"\n",
        "    }"
      ],
      "metadata": {
        "id": "nJWjkLkB3p0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "sr = 50\n",
        "qa_dict = {\"author\": \"\",\n",
        "    \"version\": \"\",\n",
        "    \"date\": str(datetime.now().date()),\n",
        "    \"dataset\": []\n",
        "}\n"
      ],
      "metadata": {
        "id": "pHaNe-6f3p0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Don't run this everytime as this cell will take long time to run (model train)\n",
        "i = 0\n",
        "for d in all_train_segments:\n",
        "    assert len(d[0])==15\n",
        "    c_acc_x = d[:, 0]\n",
        "    c_acc_y = d[:, 1]\n",
        "    c_acc_z = d[:, 2]\n",
        "    la_acc_x = d[:, 3]\n",
        "    la_acc_y = d[:, 4]\n",
        "    la_acc_z = d[:, 5]\n",
        "    la_gs_x = d[:, 6]\n",
        "    la_gs_y = d[:, 7]\n",
        "    la_gs_z = d[:, 8]\n",
        "    rla_acc_x = d[:, 9]\n",
        "    rla_acc_y = d[:, 10]\n",
        "    rla_acc_z = d[:, 11]\n",
        "    rla_gs_x = d[:, 12]\n",
        "    rla_gs_y = d[:, 13]\n",
        "    rla_gs_z = d[:, 14]\n",
        "    reading_list = [c_acc_x, c_acc_y, c_acc_z, la_acc_x, la_acc_y, la_acc_z, la_gs_x, la_gs_y, la_gs_z, rla_acc_x, rla_acc_y, rla_acc_z, rla_gs_x, rla_gs_y, rla_gs_z]\n",
        "    reading_name = [\"chest x-axis accelerometer\", \"chest y-axis accelerometer\", \"chest z-axis accelerometer\",\n",
        "                    \"left-ankle x-axis accelerometer\", \"left-ankle y-axis accelerometer\", \"left-ankle z-axis accelerometer\",\n",
        "                    \"left-ankle x-axis gyroscope\", \"left-ankle y-axis gyroscope\", \"left-ankle z-axis gyroscope\",\n",
        "                    \"right-lower-arm x-axis accelerometer\", \"right-lower-arm y-axis accelerometer\", \"right-lower-arm z-axis accelerometer\",\n",
        "                    \"right-lower-arm x-axis gyroscope\", \"right-lower-arm y-axis gyroscope\", \"right-lower-arm z-axis gyroscope\"]\n",
        "\n",
        "    data_dict = {\n",
        "        \"index\": i,\n",
        "        \"summaries\": {},\n",
        "        \"qa_pairs\": {name: [] for name in reading_name}\n",
        "    }\n",
        "\n",
        "    for r, n in zip(reading_list, reading_name):\n",
        "        normalized_n = \"normalized \" + n\n",
        "        t_df = analyze_trend(r, sr)\n",
        "        trend_dataframe = merge_adjacent_rows(t_df)\n",
        "\n",
        "        trend_pair_list = select_random_pair()\n",
        "\n",
        "        data_dict[\"summaries\"][n] = QA_summary(r, trend_dataframe, normalized_n, trend_pair_list,\n",
        "                                               whether_gpt=False, model_type='3.5')\n",
        "        data_dict[\"qa_pairs\"][n].append(dscb_simple_trend(trend_dataframe, normalized_n, trend_pair_list,\n",
        "                                                          whether_gpt=False, model_type='4'))\n",
        "    qa_dict[\"dataset\"].append(data_dict)\n",
        "    print(f\"{i} finished\")\n",
        "    i += 1\n"
      ],
      "metadata": {
        "id": "8nEJmj2m3p0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# don't run this\n",
        "import json\n",
        "with open(os.path.join(output_path, 'train', f\"mhealth_train_qa_stage1.json\"), 'w') as f:\n",
        "    json.dump(qa_dict, f, indent=2)\n",
        "print(len(qa_dict[\"dataset\"]))"
      ],
      "metadata": {
        "id": "9ksWPMJG3p0J",
        "outputId": "4d681f6b-3e50-4f35-b823-ca7fe63b2f6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Don't run this everytime as this cell will take long time to run (model test)\n",
        "qa_dict = {\"author\": \"\",\n",
        "    \"version\": \"\",\n",
        "    \"date\": str(datetime.now().date()),\n",
        "    \"dataset\": []\n",
        "}\n",
        "i = 0\n",
        "for d in all_test_segments:\n",
        "    assert len(d[0])==15\n",
        "    c_acc_x = d[:, 0]\n",
        "    c_acc_y = d[:, 1]\n",
        "    c_acc_z = d[:, 2]\n",
        "    la_acc_x = d[:, 3]\n",
        "    la_acc_y = d[:, 4]\n",
        "    la_acc_z = d[:, 5]\n",
        "    la_gs_x = d[:, 6]\n",
        "    la_gs_y = d[:, 7]\n",
        "    la_gs_z = d[:, 8]\n",
        "    rla_acc_x = d[:, 9]\n",
        "    rla_acc_y = d[:, 10]\n",
        "    rla_acc_z = d[:, 11]\n",
        "    rla_gs_x = d[:, 12]\n",
        "    rla_gs_y = d[:, 13]\n",
        "    rla_gs_z = d[:, 14]\n",
        "    reading_list = [c_acc_x, c_acc_y, c_acc_z, la_acc_x, la_acc_y, la_acc_z, la_gs_x, la_gs_y, la_gs_z, rla_acc_x, rla_acc_y, rla_acc_z, rla_gs_x, rla_gs_y, rla_gs_z]\n",
        "    reading_name = [\"chest x-axis accelerometer\", \"chest y-axis accelerometer\", \"chest z-axis accelerometer\",\n",
        "                    \"left-ankle x-axis accelerometer\", \"left-ankle y-axis accelerometer\", \"left-ankle z-axis accelerometer\",\n",
        "                    \"left-ankle x-axis gyroscope\", \"left-ankle y-axis gyroscope\", \"left-ankle z-axis gyroscope\",\n",
        "                    \"right-lower-arm x-axis accelerometer\", \"right-lower-arm y-axis accelerometer\", \"right-lower-arm z-axis accelerometer\",\n",
        "                    \"right-lower-arm x-axis gyroscope\", \"right-lower-arm y-axis gyroscope\", \"right-lower-arm z-axis gyroscope\"]\n",
        "\n",
        "    data_dict = {\n",
        "        \"index\": i,\n",
        "        \"summaries\": {},\n",
        "        \"qa_pairs\": {name: [] for name in reading_name}\n",
        "    }\n",
        "\n",
        "    for r, n in zip(reading_list, reading_name):\n",
        "        normalized_n = \"normalized \" + n\n",
        "        t_df = analyze_trend(r, sr)\n",
        "        trend_dataframe = merge_adjacent_rows(t_df)\n",
        "\n",
        "        trend_pair_list = select_random_pair()\n",
        "\n",
        "        data_dict[\"summaries\"][n] = QA_summary(r, trend_dataframe, normalized_n, trend_pair_list,\n",
        "                                               whether_gpt=False, model_type='3.5')\n",
        "        data_dict[\"qa_pairs\"][n].append(dscb_simple_trend(trend_dataframe, normalized_n, trend_pair_list,\n",
        "                                                          whether_gpt=False, model_type='4'))\n",
        "    qa_dict[\"dataset\"].append(data_dict)\n",
        "    print(f\"{i} finished\")\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "ztEF_xjz3p0J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# don't run this\n",
        "import json\n",
        "with open(os.path.join(output_path, 'test', f\"mhealth_test_qa_stage1.json\"), 'w') as f:\n",
        "    json.dump(qa_dict, f, indent=2)\n",
        "print(len(qa_dict[\"dataset\"]))"
      ],
      "metadata": {
        "id": "fu-hZKYT3p0J",
        "outputId": "ef4ffd28-bd94-4023-cec9-da0d8784e022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.12/dist-packages (2.8.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
            "Cloning into 'SensorLLM'...\n",
            "remote: Enumerating objects: 130, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 130 (delta 9), reused 5 (delta 5), pack-reused 118 (from 1)\u001b[K\n",
            "Receiving objects: 100% (130/130), 74.73 MiB | 22.29 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n",
            "Updating files: 100% (45/45), done.\n",
            "/content/SensorLLM/SensorLLM\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torch --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers accelerate datasets evaluate sentencepiece safetensors peft bitsandbytes\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!git clone https://github.com/TanjimAnim/SensorLLM.git\n",
        "%cd SensorLLM"
      ],
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "-WYrjYIA3p0J",
        "outputId": "2dbbebee-f05f-4860-cab3-a17d0847606b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Cloning into 'Llama-3.2-1B'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 76 (delta 33), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (76/76), 2.27 MiB | 2.14 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 4.60 GiB | 19.82 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "from google.colab import userdata\n",
        "TOKEN=userdata.get('HF_TOKEN')\n",
        "!git clone https://ahmedtanjim:$TOKEN@huggingface.co/meta-llama/Llama-3.2-1B"
      ],
      "metadata": {
        "id": "NBp8EdRW3p0J",
        "outputId": "b3b55bec-12f0-45b9-cefa-66e53d4806e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/sensorllm/whole_data/train/mhealth_train_qa_stage1.json\") as f:\n",
        "    qa = json.load(f)\n",
        "print(qa['dataset'][0])\n",
        "import pickle\n",
        "with open(\"/content/drive/MyDrive/sensorllm/whole_data/train/mhealth_train_qa_stage1.json\", \"rb\") as f:\n",
        "    train_data = json.load(f)\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Ej5x1d-Ywl",
        "outputId": "f46660c3-df9f-4065-dd26-ddf02520b566"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'index': 0, 'summaries': {'chest x-axis accelerometer': {'Q': \"Can you explore the changes in the sensor data's trend from 0.0 to 1.02 seconds?\", 'A': 'Normalized chest x-axis accelerometer readings between 0.0s and 1.02s are displayed in sensor data. There are two unique trends identified in the data, which altogether have shifted direction 14 times. The input data exhibited a descending trend during the 0.58 second period, and an ascending trend for an accumulated time of 0.44 seconds. In summary, the trend is descending.', 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.58 |\\n| increase | 0.44 |', 'type': 'summary'}, 'chest y-axis accelerometer': {'Q': \"Please outline the change in sensor data's trend between 0.0s and 1.02s.\", 'A': 'The sensor data displays readings obtained from a normalized chest y-axis accelerometer sensor from 0.0 seconds to 1.02 seconds. 2 varied trends have been observed in the data, which altogether experienced 23 transitions. Summarily, a declining direction was evident across 0.56 seconds of data, and a growing trend observed over 0.46 seconds. Overall, the trend is declining.', 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.56 |\\n| increase | 0.46 |', 'type': 'summary'}, 'chest z-axis accelerometer': {'Q': 'Please provide an overview of how the sensor data trends evolved from 0.0 seconds to 1.02 seconds.', 'A': \"The sensor data contains normalized chest z-axis accelerometer sensor data between 0.0s and 1.02s. The data highlights two significant trends, while also indicating that the trend has changed twenty times overall. To encapsulate, the data's decreasing trend spanned a combined duration of 0.50 seconds, and an increasing pattern for an aggregate time of 0.52 seconds. The general trend shows increasing.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.5 |\\n| increase | 0.52 |', 'type': 'summary'}, 'left-ankle x-axis accelerometer': {'Q': \"Please describe how the input time series data's trends changed from 0.0s to 1.02s.\", 'A': \"Sensor data presents normalized left-ankle x-axis accelerometer data collected between 0.0 and 1.02 seconds. 2 separate trends and 26 trend shifts are seen in the data. To encapsulate the findings, the data's falling behavior lasted for a cumulative period of 0.54 seconds, and a rising trend for 0.48 seconds. Key observation: the overall trend is rising.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.54 |\\n| increase | 0.48 |', 'type': 'summary'}, 'left-ankle y-axis accelerometer': {'Q': \"I'm looking for a summary of the time series data trends between 0.0 and 1.02 seconds. What can you tell me?\", 'A': \"The sensor data displays readings obtained from a normalized left-ankle y-axis accelerometer sensor from 0.0 seconds to 1.02 seconds. The data indicates two primary shifting trends, with these trends transforming a total of twenty eight times. The analysis concludes that the data's descending trend had a total lifespan of 0.42 seconds, and an ascending trend for an accumulated time of 0.60 seconds. The trend overall is descending.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.42 |\\n| increase | 0.6 |', 'type': 'summary'}, 'left-ankle z-axis accelerometer': {'Q': \"Explain the sensor data's trend shifts from 0.0s to 1.02s.\", 'A': 'The sensor data displays readings obtained from a normalized left-ankle z-axis accelerometer sensor from 0.0 seconds to 1.02 seconds. The data reveals 2 distinct trends with 27 trend variations. In total, the data showed a decreasing trend lasting 0.56 seconds, and an increasing pattern for 0.46 seconds in total. The general trend shows increasing.', 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.56 |\\n| increase | 0.46 |', 'type': 'summary'}, 'left-ankle x-axis gyroscope': {'Q': 'Kindly analyze the sensor data trend variations between 0.0 and 1.02 seconds.', 'A': \"Readings from a normalized left-ankle x-axis gyroscope sensor, captured from 0.0 seconds to 1.02 seconds, are depicted in the given time series data. The analysis points to three distinct trends and thirty changes in the trends. Recapitulating, the data's decreasing tendency endured for an aggregate timeframe of 0.18 seconds, trailed by an increasing trend for a cumulative period of 0.12 seconds, and a consistent trend for 0.72 seconds. The overarching trend is identified as decreasing.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.18 |\\n| increase | 0.12 |\\n| steady | 0.72 |', 'type': 'summary'}, 'left-ankle y-axis gyroscope': {'Q': \"Summarize the time series data's trend developments between 0.0 and 1.02 seconds.\", 'A': \"Readings collected from a normalized left-ankle y-axis gyroscope sensor from 0.0s to 1.02s are documented in this sensor data. The data mirrors three different development tendencies, while also illustrating that the trend has changed 30 times in total. Recapitulating, the data's decreasing tendency endured for an aggregate timeframe of 0.18 seconds, trailed by an increasing trend observed over 0.12 seconds, and a steady trend for an accumulated time of 0.72 seconds. The general trend is decreasing.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.18 |\\n| increase | 0.12 |\\n| steady | 0.72 |', 'type': 'summary'}, 'left-ankle z-axis gyroscope': {'Q': 'I would be grateful if you could provide a description of the sensor data trend alterations that took place from 0.0s to 1.02s.', 'A': \"The sensor data shows normalized left-ankle z-axis gyroscope readings from 0.0s to 1.02s. From a holistic perspective, the data presents three unique trend forms, which have undergone 28 changes throughout the process. To encapsulate, the data's downward trend spanned a combined duration of 0.10 seconds, and then a pattern of upward for 0.18 seconds, and a consistent pattern for 0.74 seconds. Key observation: the overall trend is upward.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.1 |\\n| increase | 0.18 |\\n| steady | 0.74 |', 'type': 'summary'}, 'right-lower-arm x-axis accelerometer': {'Q': 'I would be grateful if you could provide a description of the sensor data trend alterations that took place from 0.0s to 1.02s.', 'A': \"The sensor data provided represents the output of a normalized right-lower-arm x-axis accelerometer sensor recorded between 0.0s and 1.02s. There are two unique trends and fifteen total trend changes observed in the data. In a nutshell, the data's decreasing propensity persisted for an accumulated duration of 0.44 seconds, and an increasing pattern for a sum of 0.58 seconds. The dominant trend is increasing.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.44 |\\n| increase | 0.58 |', 'type': 'summary'}, 'right-lower-arm y-axis accelerometer': {'Q': 'Could you examine the shift in sensor data trends from 0.0 seconds to 1.02 seconds?', 'A': \"The time series data represent the normalized right-lower-arm y-axis accelerometer sensor's measurements taken from 0.0 seconds to 1.02 seconds. Observation indicates three different trends with 8 instances of trend changes. To sum up, the data exhibited a descending trend for a cumulative period of 0.46 seconds, trailed by an ascending pattern observed over 0.54 seconds, and a stable trend observed over 0.02 seconds. Overall, the trend is ascending.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.46 |\\n| increase | 0.54 |\\n| steady | 0.02 |', 'type': 'summary'}, 'right-lower-arm z-axis accelerometer': {'Q': \"Please describe how the input time series data's trends changed from 0.0s to 1.02s.\", 'A': \"The time series data illustrates the normalized right-lower-arm z-axis accelerometer sensor's measurements captured from 0.0s to 1.02s. Analysis reveals two separate trends within the data, undergoing a cumulative total of 18 shifts in direction. In conclusion, the overall timespan of the data's decreasing tendency amounted to 0.54 seconds, and an increasing pattern for an accumulated time of 0.48 seconds. The general trend observed is increasing.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.54 |\\n| increase | 0.48 |', 'type': 'summary'}, 'right-lower-arm x-axis gyroscope': {'Q': 'I would be grateful if you could provide a description of the time series data trend alterations that took place from 0.0s to 1.02s.', 'A': \"The sensor data under consideration contains the normalized right-lower-arm x-axis gyroscope sensor's output captured from 0.0 seconds to 1.02 seconds. In the data, three distinct movement trends are evident, and there have been 28 total trend alterations. The investigation concludes that the data's decreasing trend had a total lifespan of 0.14 seconds, trailed by a sequence of increasing occurring over 0.14 seconds, and a constant trend for a cumulative period of 0.74 seconds. The general trend is increasing.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.14 |\\n| increase | 0.14 |\\n| steady | 0.74 |', 'type': 'summary'}, 'right-lower-arm y-axis gyroscope': {'Q': \"Can you explore the changes in the time series data's trend from 0.0 to 1.02 seconds?\", 'A': \"The presented time series data encapsulates the normalized right-lower-arm y-axis gyroscope sensor's readings collected sequentially between 0.0s and 1.02s. The data contains 3 trends, exhibiting 30 trend modifications. To epitomize, the data's falling characteristic persevered for a sum of 0.12 seconds, trailed by a rising pattern for an aggregate time of 0.18 seconds, and a steady pattern for an accumulated time of 0.72 seconds. In summary, the overall trend is rising.\", 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.12 |\\n| increase | 0.18 |\\n| steady | 0.72 |', 'type': 'summary'}, 'right-lower-arm z-axis gyroscope': {'Q': \"What were the sensor data's trend modifications from 0.0 to 1.02 seconds?\", 'A': 'This sensor data represents the readings from a normalized right-lower-arm z-axis gyroscope sensor between 0.0s and 1.02s. There are three unique trends and thirty total trend changes observed in the data. The input data exhibited a declining trend during the 0.16 second period, trailed by a growing pattern observed over 0.14 seconds, and a consistent pattern for a sum of 0.72 seconds. The predominant trend is observed to be declining.', 'smry_table': '| trend | total_time |\\n|---|---|\\n| decrease | 0.16 |\\n| increase | 0.14 |\\n| steady | 0.72 |', 'type': 'summary'}}, 'qa_pairs': {'chest x-axis accelerometer': [{'Q': 'Explain the trend developments in the sensor data.', 'A': '0.0-0.14s: ascending\\n0.14-0.3s: descending\\n0.3-0.32s: ascending\\n0.32-0.4s: descending\\n0.4-0.56s: ascending\\n0.56-0.64s: descending\\n0.64-0.66s: ascending\\n0.66-0.68s: descending\\n0.68-0.74s: ascending\\n0.74-0.76s: descending\\n0.76-0.78s: ascending\\n0.78-0.96s: descending\\n0.96-0.98s: ascending\\n0.98-1.02s: descending\\nCount of ascending segments: 7\\nCount of descending segments: 7', 'type': 'simple_trend'}], 'chest y-axis accelerometer': [{'Q': 'Please explore the time series data for me, highlighting any significant trends and changes that have occurred.', 'A': '0.0-0.04s: declining\\n0.04-0.18s: growing\\n0.18-0.2s: declining\\n0.2-0.22s: growing\\n0.22-0.24s: declining\\n0.24-0.32s: growing\\n0.32-0.4s: declining\\n0.4-0.42s: growing\\n0.42-0.52s: declining\\n0.52-0.56s: growing\\n0.56-0.6s: declining\\n0.6-0.62s: growing\\n0.62-0.66s: declining\\n0.66-0.68s: growing\\n0.68-0.7s: declining\\n0.7-0.72s: growing\\n0.72-0.74s: declining\\n0.74-0.8s: growing\\n0.8-0.82s: declining\\n0.82-0.84s: growing\\n0.84-0.98s: declining\\n0.98-1.0s: growing\\n1.0-1.02s: declining\\nNumber of declining trends: 12\\nNumber of growing trends: 11', 'type': 'simple_trend'}], 'chest z-axis accelerometer': [{'Q': 'Please offer a comprehensive description of how the trends in the sensor data have evolved.', 'A': '0.0s to 0.02s: increasing\\n0.02s to 0.04s: decreasing\\n0.04s to 0.08s: increasing\\n0.08s to 0.16s: decreasing\\n0.16s to 0.2s: increasing\\n0.2s to 0.22s: decreasing\\n0.22s to 0.3s: increasing\\n0.3s to 0.46s: decreasing\\n0.46s to 0.48s: increasing\\n0.48s to 0.52s: decreasing\\n0.52s to 0.68s: increasing\\n0.68s to 0.72s: decreasing\\n0.72s to 0.74s: increasing\\n0.74s to 0.76s: decreasing\\n0.76s to 0.78s: increasing\\n0.78s to 0.82s: decreasing\\n0.82s to 0.84s: increasing\\n0.84s to 0.86s: decreasing\\n0.86s to 0.96s: increasing\\n0.96s to 1.02s: decreasing\\nNumber of increasing segments: 10\\nNumber of decreasing segments: 10', 'type': 'simple_trend'}], 'left-ankle x-axis accelerometer': [{'Q': 'Could you analyze the trends observed in the time series data over the specified period step by step?', 'A': '0.0s to 0.02s: rising\\n0.02s to 0.04s: falling\\n0.04s to 0.08s: rising\\n0.08s to 0.12s: falling\\n0.12s to 0.14s: rising\\n0.14s to 0.24s: falling\\n0.24s to 0.34s: rising\\n0.34s to 0.42s: falling\\n0.42s to 0.44s: rising\\n0.44s to 0.5s: falling\\n0.5s to 0.54s: rising\\n0.54s to 0.56s: falling\\n0.56s to 0.62s: rising\\n0.62s to 0.64s: falling\\n0.64s to 0.68s: rising\\n0.68s to 0.7s: falling\\n0.7s to 0.74s: rising\\n0.74s to 0.78s: falling\\n0.78s to 0.8s: rising\\n0.8s to 0.82s: falling\\n0.82s to 0.84s: rising\\n0.84s to 0.88s: falling\\n0.88s to 0.9s: rising\\n0.9s to 0.94s: falling\\n0.94s to 0.98s: rising\\n0.98s to 1.02s: falling\\nCount of rising trends: 13\\nCount of falling trends: 13', 'type': 'simple_trend'}], 'left-ankle y-axis accelerometer': [{'Q': 'Please analyze the trend shifts in the time series data.', 'A': '0.0 to 0.1 seconds: ascending\\n0.1 to 0.12 seconds: descending\\n0.12 to 0.16 seconds: ascending\\n0.16 to 0.18 seconds: descending\\n0.18 to 0.26 seconds: ascending\\n0.26 to 0.28 seconds: descending\\n0.28 to 0.3 seconds: ascending\\n0.3 to 0.32 seconds: descending\\n0.32 to 0.36 seconds: ascending\\n0.36 to 0.4 seconds: descending\\n0.4 to 0.42 seconds: ascending\\n0.42 to 0.44 seconds: descending\\n0.44 to 0.5 seconds: ascending\\n0.5 to 0.54 seconds: descending\\n0.54 to 0.64 seconds: ascending\\n0.64 to 0.66 seconds: descending\\n0.66 to 0.7 seconds: ascending\\n0.7 to 0.74 seconds: descending\\n0.74 to 0.76 seconds: ascending\\n0.76 to 0.82 seconds: descending\\n0.82 to 0.84 seconds: ascending\\n0.84 to 0.86 seconds: descending\\n0.86 to 0.88 seconds: ascending\\n0.88 to 0.9 seconds: descending\\n0.9 to 0.92 seconds: ascending\\n0.92 to 0.94 seconds: descending\\n0.94 to 0.96 seconds: ascending\\n0.96 to 1.02 seconds: descending\\nNumber of ascending segments: 14\\nNumber of descending segments: 14', 'type': 'simple_trend'}], 'left-ankle z-axis accelerometer': [{'Q': \"I'm looking for an in-depth examination of the sensor data. Could you elucidate the trends and pivotal changes?\", 'A': '0.0 seconds to 0.04 seconds: increasing\\n0.04 seconds to 0.06 seconds: decreasing\\n0.06 seconds to 0.08 seconds: increasing\\n0.08 seconds to 0.1 seconds: decreasing\\n0.1 seconds to 0.12 seconds: increasing\\n0.12 seconds to 0.16 seconds: decreasing\\n0.16 seconds to 0.18 seconds: increasing\\n0.18 seconds to 0.38 seconds: decreasing\\n0.38 seconds to 0.4 seconds: increasing\\n0.4 seconds to 0.44 seconds: decreasing\\n0.44 seconds to 0.48 seconds: increasing\\n0.48 seconds to 0.5 seconds: decreasing\\n0.5 seconds to 0.54 seconds: increasing\\n0.54 seconds to 0.56 seconds: decreasing\\n0.56 seconds to 0.6 seconds: increasing\\n0.6 seconds to 0.62 seconds: decreasing\\n0.62 seconds to 0.66 seconds: increasing\\n0.66 seconds to 0.68 seconds: decreasing\\n0.68 seconds to 0.74 seconds: increasing\\n0.74 seconds to 0.76 seconds: decreasing\\n0.76 seconds to 0.82 seconds: increasing\\n0.82 seconds to 0.86 seconds: decreasing\\n0.86 seconds to 0.88 seconds: increasing\\n0.88 seconds to 0.9 seconds: decreasing\\n0.9 seconds to 0.92 seconds: increasing\\n0.92 seconds to 1.0 seconds: decreasing\\n1.0 seconds to 1.02 seconds: increasing\\nTotal increasing segments: 14\\nTotal decreasing segments: 13', 'type': 'simple_trend'}], 'left-ankle x-axis gyroscope': [{'Q': 'Could you kindly assess the sensor data and provide a description of the trend transformations that took place step by step?', 'A': '0.0 to 0.02 seconds: decreasing\\n0.02 to 0.06 seconds: consistent\\n0.06 to 0.08 seconds: decreasing\\n0.08 to 0.12 seconds: consistent\\n0.12 to 0.14 seconds: decreasing\\n0.14 to 0.2 seconds: consistent\\n0.2 to 0.22 seconds: decreasing\\n0.22 to 0.26 seconds: consistent\\n0.26 to 0.28 seconds: decreasing\\n0.28 to 0.34 seconds: consistent\\n0.34 to 0.36 seconds: decreasing\\n0.36 to 0.4 seconds: consistent\\n0.4 to 0.42 seconds: increasing\\n0.42 to 0.46 seconds: consistent\\n0.46 to 0.48 seconds: increasing\\n0.48 to 0.54 seconds: consistent\\n0.54 to 0.56 seconds: increasing\\n0.56 to 0.6 seconds: consistent\\n0.6 to 0.62 seconds: decreasing\\n0.62 to 0.68 seconds: consistent\\n0.68 to 0.7 seconds: increasing\\n0.7 to 0.74 seconds: consistent\\n0.74 to 0.76 seconds: increasing\\n0.76 to 0.82 seconds: consistent\\n0.82 to 0.84 seconds: decreasing\\n0.84 to 0.88 seconds: consistent\\n0.88 to 0.9 seconds: increasing\\n0.9 to 0.94 seconds: consistent\\n0.94 to 0.96 seconds: decreasing\\n0.96 to 1.02 seconds: consistent\\nTotal consistent trends: 15\\nTotal decreasing trends: 9\\nTotal increasing trends: 6', 'type': 'simple_trend'}], 'left-ankle y-axis gyroscope': [{'Q': 'Could you kindly assess the sensor data and provide a description of the trend transformations that took place step by step?', 'A': '0.0 to 0.02 seconds: decreasing\\n0.02 to 0.06 seconds: steady\\n0.06 to 0.08 seconds: decreasing\\n0.08 to 0.12 seconds: steady\\n0.12 to 0.14 seconds: decreasing\\n0.14 to 0.2 seconds: steady\\n0.2 to 0.22 seconds: increasing\\n0.22 to 0.26 seconds: steady\\n0.26 to 0.28 seconds: decreasing\\n0.28 to 0.34 seconds: steady\\n0.34 to 0.36 seconds: decreasing\\n0.36 to 0.4 seconds: steady\\n0.4 to 0.42 seconds: decreasing\\n0.42 to 0.46 seconds: steady\\n0.46 to 0.48 seconds: decreasing\\n0.48 to 0.54 seconds: steady\\n0.54 to 0.56 seconds: increasing\\n0.56 to 0.6 seconds: steady\\n0.6 to 0.62 seconds: decreasing\\n0.62 to 0.68 seconds: steady\\n0.68 to 0.7 seconds: increasing\\n0.7 to 0.74 seconds: steady\\n0.74 to 0.76 seconds: increasing\\n0.76 to 0.82 seconds: steady\\n0.82 to 0.84 seconds: increasing\\n0.84 to 0.88 seconds: steady\\n0.88 to 0.9 seconds: increasing\\n0.9 to 0.94 seconds: steady\\n0.94 to 0.96 seconds: decreasing\\n0.96 to 1.02 seconds: steady\\nCount of steady trends: 15\\nCount of decreasing trends: 9\\nCount of increasing trends: 6', 'type': 'simple_trend'}], 'left-ankle z-axis gyroscope': [{'Q': 'Offer insights into the trend alterations within the sensor data.', 'A': '0.0s to 0.02s: downward\\n0.02s to 0.06s: consistent\\n0.06s to 0.08s: downward\\n0.08s to 0.12s: consistent\\n0.12s to 0.14s: downward\\n0.14s to 0.2s: consistent\\n0.2s to 0.22s: downward\\n0.22s to 0.26s: consistent\\n0.26s to 0.28s: upward\\n0.28s to 0.34s: consistent\\n0.34s to 0.36s: upward\\n0.36s to 0.4s: consistent\\n0.4s to 0.42s: downward\\n0.42s to 0.46s: consistent\\n0.46s to 0.48s: upward\\n0.48s to 0.54s: consistent\\n0.54s to 0.56s: upward\\n0.56s to 0.6s: consistent\\n0.6s to 0.62s: upward\\n0.62s to 0.68s: consistent\\n0.68s to 0.7s: upward\\n0.7s to 0.74s: consistent\\n0.74s to 0.76s: upward\\n0.76s to 0.82s: consistent\\n0.82s to 0.84s: upward\\n0.84s to 0.94s: consistent\\n0.94s to 0.96s: upward\\n0.96s to 1.02s: consistent\\nTotal consistent trends: 14\\nTotal upward trends: 9\\nTotal downward trends: 5', 'type': 'simple_trend'}], 'right-lower-arm x-axis accelerometer': [{'Q': 'Explain the trend developments in the time series data.', 'A': '0.0s to 0.02s: decreasing\\n0.02s to 0.22s: increasing\\n0.22s to 0.24s: decreasing\\n0.24s to 0.26s: increasing\\n0.26s to 0.38s: decreasing\\n0.38s to 0.4s: increasing\\n0.4s to 0.44s: decreasing\\n0.44s to 0.6s: increasing\\n0.6s to 0.7s: decreasing\\n0.7s to 0.8s: increasing\\n0.8s to 0.88s: decreasing\\n0.88s to 0.9s: increasing\\n0.9s to 0.92s: decreasing\\n0.92s to 0.98s: increasing\\n0.98s to 1.02s: decreasing\\nTotal decreasing trends: 8\\nTotal increasing trends: 7', 'type': 'simple_trend'}], 'right-lower-arm y-axis accelerometer': [{'Q': 'Give a brief analysis of sensor data trend developments.', 'A': '0.0s-0.02s: descending\\n0.02s-0.26s: ascending\\n0.26s-0.54s: descending\\n0.54s-0.56s: stable\\n0.56s-0.68s: ascending\\n0.68s-0.74s: descending\\n0.74s-0.92s: ascending\\n0.92s-1.02s: descending\\nTotal descending trends: 4\\nTotal ascending trends: 3\\nTotal stable trends: 1', 'type': 'simple_trend'}], 'right-lower-arm z-axis accelerometer': [{'Q': 'Could you analyze the trends observed in the sensor data over the specified period step by step?', 'A': '0.0s-0.02s: decreasing\\n0.02s-0.04s: increasing\\n0.04s-0.06s: decreasing\\n0.06s-0.1s: increasing\\n0.1s-0.12s: decreasing\\n0.12s-0.14s: increasing\\n0.14s-0.24s: decreasing\\n0.24s-0.26s: increasing\\n0.26s-0.32s: decreasing\\n0.32s-0.36s: increasing\\n0.36s-0.38s: decreasing\\n0.38s-0.5s: increasing\\n0.5s-0.56s: decreasing\\n0.56s-0.68s: increasing\\n0.68s-0.76s: decreasing\\n0.76s-0.78s: increasing\\n0.78s-0.94s: decreasing\\n0.94s-1.02s: increasing\\nNumber of decreasing segments: 9\\nNumber of increasing segments: 9', 'type': 'simple_trend'}], 'right-lower-arm x-axis gyroscope': [{'Q': 'Please analyze the trend changes in the sensor data.', 'A': '0.0 seconds to 0.02 seconds: increasing\\n0.02 seconds to 0.08 seconds: constant\\n0.08 seconds to 0.1 seconds: decreasing\\n0.1 seconds to 0.14 seconds: constant\\n0.14 seconds to 0.16 seconds: decreasing\\n0.16 seconds to 0.28 seconds: constant\\n0.28 seconds to 0.3 seconds: increasing\\n0.3 seconds to 0.36 seconds: constant\\n0.36 seconds to 0.38 seconds: decreasing\\n0.38 seconds to 0.42 seconds: constant\\n0.42 seconds to 0.44 seconds: increasing\\n0.44 seconds to 0.5 seconds: constant\\n0.5 seconds to 0.52 seconds: increasing\\n0.52 seconds to 0.56 seconds: constant\\n0.56 seconds to 0.58 seconds: increasing\\n0.58 seconds to 0.64 seconds: constant\\n0.64 seconds to 0.66 seconds: increasing\\n0.66 seconds to 0.7 seconds: constant\\n0.7 seconds to 0.72 seconds: increasing\\n0.72 seconds to 0.78 seconds: constant\\n0.78 seconds to 0.8 seconds: decreasing\\n0.8 seconds to 0.84 seconds: constant\\n0.84 seconds to 0.86 seconds: decreasing\\n0.86 seconds to 0.92 seconds: constant\\n0.92 seconds to 0.94 seconds: decreasing\\n0.94 seconds to 0.98 seconds: constant\\n0.98 seconds to 1.0 seconds: decreasing\\n1.0 seconds to 1.02 seconds: constant\\nTotal constant segments: 14\\nTotal increasing segments: 7\\nTotal decreasing segments: 7', 'type': 'simple_trend'}], 'right-lower-arm y-axis gyroscope': [{'Q': 'Can you dissect the time series data and explain the trend changes in a detailed manner?', 'A': '0.0s-0.02s: falling\\n0.02s-0.08s: steady\\n0.08s-0.1s: rising\\n0.1s-0.14s: steady\\n0.14s-0.16s: rising\\n0.16s-0.22s: steady\\n0.22s-0.24s: rising\\n0.24s-0.28s: steady\\n0.28s-0.3s: rising\\n0.3s-0.36s: steady\\n0.36s-0.38s: rising\\n0.38s-0.42s: steady\\n0.42s-0.44s: falling\\n0.44s-0.5s: steady\\n0.5s-0.52s: falling\\n0.52s-0.56s: steady\\n0.56s-0.58s: falling\\n0.58s-0.64s: steady\\n0.64s-0.66s: falling\\n0.66s-0.7s: steady\\n0.7s-0.72s: falling\\n0.72s-0.78s: steady\\n0.78s-0.8s: rising\\n0.8s-0.84s: steady\\n0.84s-0.86s: rising\\n0.86s-0.92s: steady\\n0.92s-0.94s: rising\\n0.94s-0.98s: steady\\n0.98s-1.0s: rising\\n1.0s-1.02s: steady\\nCount of steady segments: 15\\nCount of rising segments: 9\\nCount of falling segments: 6', 'type': 'simple_trend'}], 'right-lower-arm z-axis gyroscope': [{'Q': 'Could you analyze the trends observed in the sensor data over the specified period step by step?', 'A': '0.0 to 0.02 seconds: declining\\n0.02 to 0.08 seconds: consistent\\n0.08 to 0.1 seconds: declining\\n0.1 to 0.14 seconds: consistent\\n0.14 to 0.16 seconds: growing\\n0.16 to 0.22 seconds: consistent\\n0.22 to 0.24 seconds: growing\\n0.24 to 0.28 seconds: consistent\\n0.28 to 0.3 seconds: declining\\n0.3 to 0.36 seconds: consistent\\n0.36 to 0.38 seconds: growing\\n0.38 to 0.42 seconds: consistent\\n0.42 to 0.44 seconds: declining\\n0.44 to 0.5 seconds: consistent\\n0.5 to 0.52 seconds: growing\\n0.52 to 0.56 seconds: consistent\\n0.56 to 0.58 seconds: declining\\n0.58 to 0.64 seconds: consistent\\n0.64 to 0.66 seconds: declining\\n0.66 to 0.7 seconds: consistent\\n0.7 to 0.72 seconds: declining\\n0.72 to 0.78 seconds: consistent\\n0.78 to 0.8 seconds: growing\\n0.8 to 0.84 seconds: consistent\\n0.84 to 0.86 seconds: declining\\n0.86 to 0.92 seconds: consistent\\n0.92 to 0.94 seconds: growing\\n0.94 to 0.98 seconds: consistent\\n0.98 to 1.0 seconds: growing\\n1.0 to 1.02 seconds: consistent\\nCount of consistent segments: 15\\nCount of declining segments: 8\\nCount of growing segments: 7', 'type': 'simple_trend'}]}}\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"/content/output\", exist_ok=True)"
      ],
      "metadata": {
        "id": "CG9vpbNEtj4x"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running only the command\n",
        "\n",
        "!python -m sensorllm.train.train_mem \\\n",
        "  --model_name_or_path ../SensorLLM/Llama-3.2-1B/ \\\n",
        "  --pt_encoder_backbone_ckpt amazon/chronos-t5-base \\\n",
        "  --tokenize_method \"StanNormalizeUniformBins\" \\\n",
        "  --dataset mhealth \\\n",
        "  --data_path /content/drive/MyDrive/sensorllm/whole_data/train/mhealth_train_data_stage1.pkl \\\n",
        "  --eval_data_path /content/drive/MyDrive/sensorllm/whole_data/test/mhealth_test_data_stage1.pkl \\\n",
        "  --qa_path /content/drive/MyDrive/sensorllm/whole_data/train/mhealth_train_qa_stage1.json \\\n",
        "  --eval_qa_path /content/drive/MyDrive/sensorllm/whole_data/test/mhealth_test_qa_stage1.json \\\n",
        "  --output_dir /content/output \\\n",
        "  --model_max_length 1024 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --per_device_eval_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --eval_strategy steps \\\n",
        "  --save_strategy steps \\\n",
        "  --save_steps 100 \\\n",
        "  --eval_steps 100 \\\n",
        "  --learning_rate 2e-4 \\\n",
        "  --weight_decay 0.0 \\\n",
        "  --warmup_ratio 0.03 \\\n",
        "  --lr_scheduler_type cosine \\\n",
        "  --logging_steps 1 \\\n",
        "  --gradient_checkpointing True \\\n",
        "  --save_total_limit 1 \\\n",
        "  --bf16 False \\\n",
        "  --fix_llm True \\\n",
        "  --fix_ts_encoder True \\\n",
        "  --model_type CasualLM \\\n",
        "  --load_best_model_at_end True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HALQ4RhGtqKJ",
        "outputId": "c1b195a6-a5e9-4051-bc1b-097218c63b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-19 16:49:41.284428: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763570981.311567   21343 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763570981.321122   21343 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763570981.352063   21343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763570981.352096   21343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763570981.352099   21343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763570981.352103   21343 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-19 16:49:41.356929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}